{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5840\\3461678202.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\radiomics\\lib\\site-packages\\pandas\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig_init\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m from pandas.core.api import (\n\u001b[0m\u001b[0;32m     51\u001b[0m     \u001b[1;31m# dtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[0mInt8Dtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\radiomics\\lib\\site-packages\\pandas\\core\\api.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[0mvalue_counts\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m )\n\u001b[1;32m---> 29\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marrays\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCategorical\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mboolean\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBooleanDtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m from pandas.core.arrays.floating import (\n",
      "\u001b[1;32m~\\.conda\\envs\\radiomics\\lib\\site-packages\\pandas\\core\\arrays\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mboolean\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBooleanArray\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcategorical\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCategorical\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatetimes\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDatetimeArray\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloating\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mFloatingArray\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minteger\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mIntegerArray\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\radiomics\\lib\\site-packages\\pandas\\core\\arrays\\datetimes.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     70\u001b[0m )\n\u001b[0;32m     71\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ranges\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgenerate_regular_range\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minteger\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mIntegerArray\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommon\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstruction\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mextract_array\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\radiomics\\lib\\site-packages\\pandas\\core\\arrays\\integer.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     43\u001b[0m )\n\u001b[0;32m     44\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0minvalid_comparison\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumeric\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mto_numeric\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\radiomics\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\radiomics\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\radiomics\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\radiomics\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\radiomics\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36mget_code\u001b[1;34m(self, fullname)\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\radiomics\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36mget_data\u001b[1;34m(self, path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import ttest_ind, levene, norm\n",
    "import pingouin as pg\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "# 分类器\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import preprocessing,tree,model_selection,svm,naive_bayes\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from xgboost import plot_importance\n",
    "from lce import LCEClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics \n",
    "from sklearn.metrics import roc_curve, roc_auc_score, classification_report, mean_squared_error, confusion_matrix\n",
    "from math import sqrt\n",
    "import csv\n",
    "import math\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(plt.style.available)\n",
    "plt.style.use('seaborn-white')\n",
    "plt.figure(dpi=300)\n",
    "# plt.xkcd() #手绘风格\n",
    "# with plt.style.context(('dark_background')) #限制在某一代码块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "xgb.set_config(verbosity=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlsx_a = 'E:/featureExtraction/Analysis/小波+Log/results/1.xlsx'\n",
    "xlsx_b = 'E:/featureExtraction/Analysis/小波+Log/results/2.xlsx'\n",
    "xlsx_c = 'E:/featureExtraction/Analysis/小波+Log/results/3.xlsx'\n",
    "data_a = pd.read_excel(xlsx_a)\n",
    "data_b = pd.read_excel(xlsx_b)\n",
    "data_c = pd.read_excel(xlsx_c)\n",
    "print(data_a.shape,data_b.shape,data_c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_a,cols_a = data_a.shape\n",
    "rows_b,cols_b = data_b.shape\n",
    "rows_c,cols_c = data_c.shape\n",
    "labels_a = np.zeros(rows_a)\n",
    "labels_b = np.ones(rows_b)\n",
    "labels_c = np.full(rows_c,2)\n",
    "data_a.insert(0, '对应结局', labels_a)\n",
    "data_b.insert(0, '对应结局', labels_b)\n",
    "data_c.insert(0, '对应结局', labels_c)\n",
    "data = pd.concat([data_a,data_b,data_c])\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test = train_test_split(data,test_size=0.25,random_state = 121)\n",
    "data_train_a = data_train[:][data_train['对应结局'] == 0]\n",
    "data_train_b = data_train[:][data_train['对应结局'] == 1]\n",
    "data_train_c = data_train[:][data_train['对应结局'] == 2]\n",
    "data_test_a = data_test[:][data_test['对应结局'] == 0]\n",
    "data_test_b = data_test[:][data_test['对应结局'] == 1]\n",
    "data_test_c = data_test[:][data_test['对应结局'] == 2]\n",
    "print(data_train_a.shape)\n",
    "print(data_train_b.shape)\n",
    "print(data_train_c.shape)\n",
    "print(data_test_a.shape)\n",
    "print(data_test_b.shape)\n",
    "print(data_test_c.shape)\n",
    "data_train_a.to_csv(\"训练集1.csv\",index=False,sep=',')\n",
    "data_train_b.to_csv(\"训练集2.csv\",index=False,sep=',')\n",
    "data_train_c.to_csv(\"训练集3.csv\",index=False,sep=',')\n",
    "data_test_a.to_csv(\"测试集1.csv\",index=False,sep=',')\n",
    "data_test_b.to_csv(\"测试集2.csv\",index=False,sep=',')\n",
    "data_test_c.to_csv(\"测试集3.csv\",index=False,sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([data_train_a,data_train_b,data_train_c])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "index = []\n",
    "for colName in data.columns[:]:\n",
    "    if levene(data_train_a[colName], data_train_b[colName], data_train_c[colName])[1] > 0.05: \n",
    "        aov = pg.anova(data=df, \n",
    "               dv=colName,  #因变量\n",
    "               between='对应结局', \n",
    "               detailed=False)\n",
    "        if aov['p-unc'].item() < 0.05: \n",
    "            index.append(colName)\n",
    "    else: \n",
    "        aov = pg.welch_anova(data=df, \n",
    "               dv=colName,  #因变量\n",
    "               between='对应结局'\n",
    "               )\n",
    "        if aov['p-unc'].item() < 0.05: \n",
    "            index.append(colName)\n",
    "print(len(index))\n",
    "print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "index.insert(0, '对应结局')\n",
    "print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_a = data_train_a[index]\n",
    "data_train_b = data_train_b[index]\n",
    "data_train_c = data_train_c[index]\n",
    "data_train = pd.concat([data_train_a, data_train_b, data_train_c])\n",
    "data_train.index = range(len(data_train))\n",
    "X_train = data_train[data_train.columns[1:]]\n",
    "# 注意下面两行在训练集与测试集上的区别\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_train = pd.DataFrame(X_train)\n",
    "X_train.columns = index[1:]\n",
    "y_train = data_train['对应结局']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test_a = data_test_a[index]\n",
    "data_test_b = data_test_b[index]\n",
    "data_test_c = data_test_c[index]\n",
    "data_test = pd.concat([data_test_a, data_test_b, data_test_c])\n",
    "data_test.index = range(len(data_test))\n",
    "X_test = data_test[data_test.columns[1:]]\n",
    "X_test = scaler.transform(X_test)# 只能用训练集的标准化方法定义\n",
    "X_test = pd.DataFrame(X_test)\n",
    "X_test.columns = index[1:]\n",
    "y_test = data_test['对应结局']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pearson_corr = data[[c for c in X_train.columns if c not in '对应结局']].corr('pearson')\n",
    "pp = sns.clustermap(pearson_corr, linewidths=.5, figsize=(50.0, 40.0), cmap='YlGnBu')\n",
    "plt.setp(pp.ax_heatmap.get_yticklabels(), rotation=0)\n",
    "plt.savefig(\"C:/Users/Ding/Desktop/Figures and tables/三分类聚类分析.svg\", dpi=300,format=\"svg\",bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hight_corr(x):\n",
    "    data_corr = x.corr()>0.8\n",
    "    a=[]\n",
    "    \n",
    "    for i in data_corr.columns:\n",
    "        if data_corr[i].sum()>=2:\n",
    "            a.append(i)\n",
    " \n",
    "    return a\n",
    "print(hight_corr(X_train))\n",
    "print(len(hight_corr(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index2 = set(index)-set(hight_corr(X_train))\n",
    "index2 = list(index2)\n",
    "index2.remove(\"对应结局\")\n",
    "print(index2)\n",
    "print(len(index2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train[index2]\n",
    "X_test = X_test[index2]\n",
    "X_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "alphas = np.logspace(-4,1,100)\n",
    "model_lassoCV = LassoCV(alphas = alphas, max_iter = 1e8, cv = 10).fit(X_train,y_train)\n",
    "coef = pd.Series(model_lassoCV.coef_, index = X_train.columns)\n",
    "print(model_lassoCV.alpha_)\n",
    "print('%s %d'%('Lasso picked',sum(abs(coef)!=0)))\n",
    "print(coef[abs(coef)!=0])\n",
    "index = coef[abs(coef)!=0].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficient=model_lassoCV.coef_[abs(model_lassoCV.coef_)!=0]\n",
    "coefficient\n",
    "model_lassoCV.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "font2 = {'family': 'Arial',\n",
    "     'weight': 'normal',\n",
    "     'size': 16, }\n",
    "Features1 = ('original_shape_Flatness','original_ngtdm_Contrast')\n",
    "Coefficient1= [0.101894, 0.023968]\n",
    "Features2 = ('wavelet-LHL_glcm_Imc2','wavelet-LHL_gldm_DependenceVariance')\n",
    "Coefficient2= [-0.017605, -0.087181]\n",
    "Features3 = ('wavelet-LLL_gldm_DependenceEntropy')\n",
    "Coefficient3= [0.094268]\n",
    "Features4 = ('log-sigma-3-mm-3D_glszm_SmallAreaEmphasis')\n",
    "Coefficient4= [-0.033748]\n",
    "plt.bar(Features1, Coefficient1, color='#ff073a')  # 横放条形图函数 barh\n",
    "plt.bar(Features2, Coefficient2, color='#1f77b4')\n",
    "plt.bar(Features3, Coefficient3, color='#ff073a')  \n",
    "plt.bar(Features4, Coefficient4, color='#1f77b4')\n",
    "# 添加竖线\n",
    "# plt.axvline(x=0, linestyle='-', color='black')\n",
    "plt.xticks(fontsize=14,family='Arial', rotation=45,ha = 'right',va = 'top')\n",
    "plt.yticks(fontsize=14,family='Arial')\n",
    "plt.ylabel('Lasso regression coefficient',font2)\n",
    "plt.savefig(\"C:/Users/Ding/Desktop/Figures and tables/三分类特征权重图.svg\", dpi=300,format=\"svg\",bbox_inches = 'tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,5), dpi= 300)\n",
    "corr = X_train.corr()\n",
    "mask = np.zeros_like(corr)\n",
    "print(mask)\n",
    "mask[np.triu_indices_from(mask,k=1)] = True   # 抽取上三角矩阵\n",
    "print(mask)\n",
    "sns.heatmap(corr                           #计算特征间的相关性\n",
    "            , xticklabels=X_train.corr().columns\n",
    "            , yticklabels=X_train.corr().columns\n",
    "            , cmap='YlGnBu'\n",
    "            , center=0.35\n",
    "            , annot=True\n",
    "            , annot_kws={\"size\": 14}\n",
    "            , mask=mask\n",
    "            , square=True)\n",
    "\n",
    "plt.title('Correlogram of features', fontsize=18, family='Arial')\n",
    "plt.xticks(fontsize=14, family='Arial',rotation=45,ha = 'right',va = 'top')\n",
    "plt.yticks(fontsize=14, family='Arial')\n",
    "plt.savefig(\"C:/Users/Ding/Desktop/Figures and tables/三分类热图.svg\", dpi=300,format=\"svg\",bbox_inches = 'tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSEs = model_lassoCV.mse_path_\n",
    "\"\"\"\n",
    "MSEs_mean, MSEs_std = [], []\n",
    "for i in range(len(MSEs)):\n",
    "    MSEs_mean.append(MSEs[i].mean())\n",
    "    MSEs_std.append(MSEs[i].std())\n",
    "\"\"\"\n",
    "MSEs_mean = np.apply_along_axis(np.mean,1,MSEs)\n",
    "MSEs_std = np.apply_along_axis(np.std,1,MSEs)\n",
    "\n",
    "plt.figure()\n",
    "plt.errorbar(model_lassoCV.alphas_,MSEs_mean    #x, y数据，一一对应\n",
    "             , yerr=MSEs_std                    #y误差范围\n",
    "             , fmt=\"o\"                          #数据点标记\n",
    "             , ms=3                             #数据点大小\n",
    "             , mfc=\"r\"                          #数据点颜色\n",
    "             , mec=\"r\"                          #数据点边缘颜色\n",
    "             , ecolor=\"lightblue\"               #误差棒颜色\n",
    "             , elinewidth=1                     #误差棒线宽\n",
    "             , capsize=4                        #误差棒边界线长度\n",
    "             , capthick=0.5)                      #误差棒边界线厚度\n",
    "plt.semilogx()\n",
    "plt.axvline(model_lassoCV.alpha_,color = 'black',ls=\"--\")\n",
    "plt.xlabel('Lambda',font2)\n",
    "plt.ylabel('MSE',font2)\n",
    "plt.xticks(fontsize=14,family='Arial')\n",
    "plt.yticks(fontsize=14,family='Arial')\n",
    "plt.savefig(\"C:/Users/Ding/Desktop/Figures and tables/三分类Lasso选值图.svg\", dpi=300,format=\"svg\",bbox_inches = 'tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = model_lassoCV.path(X_train_raw,y_train,alphas = alphas, max_iter = 1e7)[1].T\n",
    "plt.figure()\n",
    "plt.semilogx(model_lassoCV.alphas_,coefs, '-')\n",
    "plt.axvline(model_lassoCV.alpha_,color = 'black',ls=\"--\")\n",
    "plt.xlabel('Lambda',font2)\n",
    "plt.ylabel('Coefficients',font2)\n",
    "plt.xticks(fontsize=14,family='Arial')\n",
    "plt.yticks(fontsize=14,family='Arial')\n",
    "plt.savefig(\"C:/Users/Ding/Desktop/Figures and tables/三分类Lasso迭代图.svg\", dpi=300,format=\"svg\",bbox_inches = 'tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index1 = ['对应结局','original_shape_Flatness','original_ngtdm_Contrast','wavelet-LHL_glcm_Imc2','wavelet-LHL_gldm_DependenceVariance',\n",
    "         'wavelet-LLL_gldm_DependenceEntropy','log-sigma-3-mm-3D_glszm_SmallAreaEmphasis']\n",
    "index2 = ['对应结局','has-miR371a-3P','has-miR-375-5P']\n",
    "index3 = ['对应结局','has-miR371a-3P']\n",
    "index4 = ['对应结局','has-miR-375-5P']\n",
    "index5 = ['original_shape_Flatness','original_ngtdm_Contrast','wavelet-LHL_glcm_Imc2','wavelet-LHL_gldm_DependenceVariance',\n",
    "         'wavelet-LLL_gldm_DependenceEntropy','log-sigma-3-mm-3D_glszm_SmallAreaEmphasis','has-miR371a-3P','has-miR-375-5P']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_a1 = data_train_a[index1]\n",
    "data_train_b1 = data_train_b[index1]\n",
    "data_train_c1 = data_train_c[index1]\n",
    "data_train1 = pd.concat([data_train_a1, data_train_b1, data_train_c1])\n",
    "X_train1 = data_train1[data_train1.columns[1:]]\n",
    "# 注意下面两行在训练集与测试集上的区别\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train1)\n",
    "X_train1 = scaler.transform(X_train1)\n",
    "X_train1 = pd.DataFrame(X_train1)\n",
    "X_train1.columns = data_train1.columns[1:]\n",
    "y_train1 = data_train1['对应结局']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test_a1 = data_test_a[index1]\n",
    "data_test_b1 = data_test_b[index1]\n",
    "data_test_c1 = data_test_c[index1]\n",
    "data_test1 = pd.concat([data_test_a1, data_test_b1, data_test_c1])\n",
    "X_test1 = data_test1[data_test1.columns[1:]]\n",
    "X_test1 = scaler.transform(X_test1)# 只能用训练集的标准化方法定义\n",
    "X_test1 = pd.DataFrame(X_test1)\n",
    "X_test1.columns = data_test1.columns[1:]\n",
    "y_test1 = data_test1['对应结局']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_a2 = data_train_a[index2]\n",
    "data_train_b2 = data_train_b[index2]\n",
    "data_train_c2 = data_train_c[index2]\n",
    "data_train2 = pd.concat([data_train_a2, data_train_b2, data_train_c2])\n",
    "X_train2 = data_train2[data_train2.columns[1:]]\n",
    "# 注意下面两行在训练集与测试集上的区别\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train2)\n",
    "X_train2 = scaler.transform(X_train2)\n",
    "X_train2 = pd.DataFrame(X_train2)\n",
    "X_train2.columns = data_train2.columns[1:]\n",
    "y_train2 = data_train2['对应结局']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test_a2 = data_test_a[index2]\n",
    "data_test_b2 = data_test_b[index2]\n",
    "data_test_c2 = data_test_c[index2]\n",
    "data_test2 = pd.concat([data_test_a2, data_test_b2, data_test_c2])\n",
    "X_test2 = data_test2[data_test2.columns[1:]]\n",
    "X_test2 = scaler.transform(X_test2)# 只能用训练集的标准化方法定义\n",
    "X_test2 = pd.DataFrame(X_test2)\n",
    "X_test2.columns = data_test2.columns[1:]\n",
    "y_test2 = data_test2['对应结局']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_a3 = data_train_a[index3]\n",
    "data_train_b3 = data_train_b[index3]\n",
    "data_train_c3 = data_train_c[index3]\n",
    "data_train3 = pd.concat([data_train_a3, data_train_b3, data_train_c3])\n",
    "X_train3 = data_train3[data_train3.columns[1:]]\n",
    "# 注意下面两行在训练集与测试集上的区别\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train3)\n",
    "X_train3 = scaler.transform(X_train3)\n",
    "X_train3 = pd.DataFrame(X_train3)\n",
    "X_train3.columns = data_train3.columns[1:]\n",
    "y_train3 = data_train3['对应结局']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test_a3 = data_test_a[index3]\n",
    "data_test_b3 = data_test_b[index3]\n",
    "data_test_c3 = data_test_c[index3]\n",
    "data_test3 = pd.concat([data_test_a3, data_test_b3, data_test_c3])\n",
    "X_test3 = data_test3[data_test3.columns[1:]]\n",
    "X_test3 = scaler.transform(X_test3)# 只能用训练集的标准化方法定义\n",
    "X_test3 = pd.DataFrame(X_test3)\n",
    "X_test3.columns = data_test3.columns[1:]\n",
    "y_test3 = data_test3['对应结局']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_a4 = data_train_a[index4]\n",
    "data_train_b4 = data_train_b[index4]\n",
    "data_train_c4 = data_train_c[index4]\n",
    "data_train4 = pd.concat([data_train_a4, data_train_b4, data_train_c4])\n",
    "X_train4 = data_train4[data_train4.columns[1:]]\n",
    "# 注意下面两行在训练集与测试集上的区别\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train4)\n",
    "X_train4 = scaler.transform(X_train4)\n",
    "X_train4 = pd.DataFrame(X_train4)\n",
    "X_train4.columns = data_train4.columns[1:]\n",
    "y_train4 = data_train4['对应结局']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test_a4 = data_test_a[index4]\n",
    "data_test_b4 = data_test_b[index4]\n",
    "data_test_c4 = data_test_c[index4]\n",
    "data_test4 = pd.concat([data_test_a4, data_test_b4, data_test_c4])\n",
    "X_test4 = data_test4[data_test4.columns[1:]]\n",
    "X_test4 = scaler.transform(X_test4)# 只能用训练集的标准化方法定义\n",
    "X_test4 = pd.DataFrame(X_test4)\n",
    "X_test4.columns = data_test4.columns[1:]\n",
    "y_test4 = data_test4['对应结局']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_a5 = data_train_a[index5]\n",
    "data_train_b5 = data_train_b[index5]\n",
    "data_train_c5 = data_train_c[index5]\n",
    "data_train5 = pd.concat([data_train_a5, data_train_b5, data_train_c5])\n",
    "X_train5 = data_train5[data_train5.columns[:]]\n",
    "# 注意下面两行在训练集与测试集上的区别\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train5)\n",
    "X_train5 = scaler.transform(X_train5)\n",
    "X_train5 = pd.DataFrame(X_train5)\n",
    "X_train5.columns = data_train5.columns[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AUC_CI(auc, label, alpha = 0.05):\n",
    "    label = np.array(label)#防止label不是array类型\n",
    "    n1, n2 = np.sum(label == 1), np.sum(label == 0)\n",
    "    q1 = auc / (2-auc)\n",
    "    q2 = (2 * auc ** 2) / (1 + auc)\n",
    "    se = np.sqrt((auc * (1 - auc) + (n1 - 1) * (q1 - auc ** 2) + (n2 -1) * (q2 - auc ** 2)) / (n1 * n2))\n",
    "    confidence_level = 1 - alpha\n",
    "    z_lower, z_upper = norm.interval(confidence_level)\n",
    "    lowerb, upperb = auc + z_lower * se, auc + z_upper * se\n",
    "    if upperb > 1:\n",
    "        upperb = 1.000\n",
    "    return (format(lowerb, '.3f'), format(upperb, '.3f'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def c_matrix(y_true, y_pred):\n",
    "    C = confusion_matrix(y_true, y_pred)\n",
    "    plt.matshow(C, cmap=plt.cm.Blues) # 根据最下面的图按自己需求更改颜色\n",
    "    plt.colorbar()\n",
    "    thresh = C.max() / 2.\n",
    "    for i in range(len(C)):\n",
    "        for j in range(len(C)):\n",
    "            plt.annotate(C[j, i], xy=(i, j), horizontalalignment='center', verticalalignment='center', color=\"white\" if C[j, i] > thresh else \"black\")\n",
    "    # plt.tick_params(labelsize=15) # 设置左边和上面的label类别如0,1,2,3,4的字体大小。\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    # plt.ylabel('True label', fontdict={'f00amily': 'Times New Roman', 'size': 20}) # 设置字体大小。\n",
    "    # plt.xlabel('Predicted label', fontdict={'family': 'Times New Roman', 'size': 20})\n",
    "    plt.xticks([0,1,2], labels=['N','T','V']) # 将x轴或y轴坐标，刻度 替换为文字/字符\n",
    "    plt.yticks([0,1,2], labels=['N','T','V'], rotation='vertical')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC曲线绘制\n",
    "def ROC(y_train1, y_train_probs):\n",
    "    font = {'family': 'Arial',\n",
    "                'size': 12,\n",
    "                }\n",
    "    sns.set(font_scale=1.2)\n",
    "    plt.rc('font',family='Arial')\n",
    "    plt.style.use('seaborn-white')\n",
    "    plt.figure(dpi=300)\n",
    "\n",
    "    global fprmacro, tprmacro, CI_train4\n",
    "    y_train = label_binarize(y_train1, classes=[0, 1, 2])\n",
    "    fpr1, tpr1, thresholds1 = roc_curve(y_train[:,0], y_train_probs[:,0], pos_label = 1)\n",
    "    fpr2, tpr2, thresholds2 = roc_curve(y_train[:,1], y_train_probs[:,1], pos_label = 1)\n",
    "    fpr3, tpr3, thresholds3 = roc_curve(y_train[:,2], y_train_probs[:,2], pos_label = 1)\n",
    "\n",
    "    all_fpr = np.unique(np.concatenate([fpr1, fpr2, fpr3]))\n",
    "    mean_tpr = np.zeros_like(all_fpr)\n",
    "    mean_tpr += np.interp(all_fpr, fpr1, tpr1)\n",
    "    mean_tpr += np.interp(all_fpr, fpr2, tpr2)\n",
    "    mean_tpr += np.interp(all_fpr, fpr3, tpr3)\n",
    "    mean_tpr /= 3\n",
    "    fprmacro = all_fpr\n",
    "    tprmacro = mean_tpr\n",
    "    fprmacro = np.insert(fprmacro,0,values=[0])\n",
    "    tprmacro = np.insert(tprmacro,0,values=[0])\n",
    "    \n",
    "    y1 = tpr1 - fpr1\n",
    "    y2 = tpr2 - fpr2\n",
    "    y3 = tpr3 - fpr3\n",
    "    y4 = tprmacro - fprmacro\n",
    "    Youden_index1 = np.argmax(y1)# Only the first occurrence is returned.\n",
    "    Youden_index2 = np.argmax(y2)\n",
    "    Youden_index3 = np.argmax(y3)\n",
    "    Youden_index4 = np.argmax(y4)\n",
    "    optimal_threshold = np.array([thresholds1[Youden_index1],thresholds2[Youden_index2],thresholds3[Youden_index3]])\n",
    "    point = np.array([[1-fpr1[Youden_index1], tpr1[Youden_index1]],[1-fpr2[Youden_index2], tpr2[Youden_index2]],[1-fpr3[Youden_index3], tpr3[Youden_index3]],[1-fprmacro[Youden_index4], tprmacro[Youden_index4]]])\n",
    "    print(optimal_threshold)\n",
    "    print('Specificity and Sensitivity')\n",
    "    print(point)\n",
    "    \n",
    "    plt.figure()\n",
    "    # plt.plot(fpr, tpr, marker = 'o')\n",
    "    auc1 = roc_auc_score(y_train[:,0], y_train_probs[:,0])\n",
    "    auc2 = roc_auc_score(y_train[:,1], y_train_probs[:,1])\n",
    "    auc3 = roc_auc_score(y_train[:,2], y_train_probs[:,2])\n",
    "    auc4 = metrics.roc_auc_score(y_train1, y_train_probs, average='macro', multi_class='ovr')\n",
    "    CI_train1=AUC_CI(auc1, y_train[:,0], alpha = 0.05)\n",
    "    CI_train2=AUC_CI(auc2, y_train[:,1], alpha = 0.05)\n",
    "    CI_train3=AUC_CI(auc3, y_train[:,2], alpha = 0.05)\n",
    "    CI_train4=[0,0]\n",
    "    CI_train4[0]=(float(CI_train1[0])+float(CI_train2[0])+float(CI_train3[0]))/3\n",
    "    CI_train4[1]=(float(CI_train1[1])+float(CI_train2[1])+float(CI_train3[1]))/3\n",
    "\n",
    "    plt.plot(fpr1,tpr1,'#1f77b4', label='{}= {} ({}-{})'.format('N vs.O_AUC', '%.3f' % auc1, CI_train1[0], CI_train1[1]))\n",
    "    plt.plot(fpr2,tpr2,'#ff7f0e', label='{}= {} ({}-{})'.format('T vs.O_AUC', '%.3f' % auc2, CI_train2[0], CI_train2[1]))\n",
    "    plt.plot(fpr3,tpr3,'#2ca02c', label='{}= {} ({}-{})'.format('V vs.O_AUC', '%.3f' % auc3, CI_train3[0], CI_train3[1]))\n",
    "    plt.plot(fprmacro, tprmacro, label='{}= {} ({}-{})'.format('macro-average ROC_AUC', '%.3f' % auc4, '%.3f' % CI_train4[0], '%.3f' % CI_train4[1]),\n",
    "             color='navy', linestyle=':', linewidth=2)\n",
    "\n",
    "    plt.xlabel(\"1 - Specificity\")\n",
    "    plt.ylabel(\"Sensitivity\")\n",
    "    plt.legend(loc='lower right',fontsize = 10)\n",
    "    plt.plot([0,1],[0,1],'k--')\n",
    "    plt.show()\n",
    "    return fprmacro, tprmacro, CI_train4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficient=[0.10189258,  0.02396634,  -0.01760259,  -0.08718162, 0.09426855, -0.03374816]\n",
    "model_lassoCV_intercept_=0.8148148148148141\n",
    "print('{}= {}{}{}*{}{}{}*{}{}{}*{}{}{}*{}{}{}*{}{}{}*{}'.format('Radscore', '%.8f' % model_lassoCV_intercept_,'+' if coefficient[0]>=0 else '',\n",
    "                                                                 '%.8f' % coefficient[0],X_train1.columns[0],'+'if coefficient[1]>=0 else '',\n",
    "                                                                 '%.8f' % coefficient[1],X_train1.columns[1],'+'if coefficient[2]>=0 else '',\n",
    "                                                                 '%.8f' % coefficient[2],X_train1.columns[2],'+'if coefficient[3]>=0 else '',\n",
    "                                                                 '%.8f' % coefficient[3],X_train1.columns[3],'+'if coefficient[4]>=0 else '',\n",
    "                                                                 '%.8f' % coefficient[4],X_train1.columns[4],'+'if coefficient[5]>=0 else '',\n",
    "                                                                 '%.8f' % coefficient[5],X_train1.columns[5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x,y):\n",
    "    return model_lassoCV_intercept_+coefficient[0]*x.iloc[y,0]+coefficient[1]*x.iloc[y,1]+coefficient[2]*x.iloc[y,2]+coefficient[3]*x.iloc[y,3]+coefficient[4]*x.iloc[y,4]+coefficient[5]*x.iloc[y,5]\n",
    "train_score = []\n",
    "for i in range(len(y_train1)):\n",
    "    a = f(X_train1,i)\n",
    "    train_score.append(a)\n",
    "dfs = pd.DataFrame(columns = [\"label\", \"Radscore\"])\n",
    "dfs['label'] = list(y_train1)\n",
    "dfs['Radscore'] = train_score\n",
    "dfs.to_csv(\"train-score_lasso三分类.csv\",index=False,sep=',')\n",
    "\n",
    "test_score = []\n",
    "for i in range(len(y_test1)):\n",
    "    a = f(X_test1,i)\n",
    "    test_score.append(a)\n",
    "dfs = pd.DataFrame(columns = [\"label\", \"Radscore\"])\n",
    "dfs['label'] = list(y_test1)\n",
    "dfs['Radscore'] = test_score\n",
    "dfs.to_csv(\"test-score_lasso三分类.csv\",index=False,sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_LR = OneVsRestClassifier(LogisticRegression(random_state = 100))\n",
    "model_LR.fit(X_train1,y_train1)\n",
    "\n",
    "predicted = model_LR.predict(X_train1)\n",
    "y_train_probs4 = model_LR.predict_proba(X_train1)\n",
    "auc4 = metrics.roc_auc_score(y_train1, y_train_probs4, average='macro', multi_class='ovr')  # auc roc曲线下面积\n",
    "accuracy = metrics.accuracy_score(y_train1, predicted)  # accuracy准确率\n",
    "Recall = metrics.recall_score(y_train1, predicted, average='weighted')   # recall 召回率\n",
    "Prec = metrics.precision_score(y_train1, predicted, average='weighted')  #precision 精度\n",
    "f1 = metrics.f1_score(y_train1, predicted, average='weighted') # f1 score f1分数\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "print(\"auc: %.2f%%\" % (auc4 * 100.0))\n",
    "print(\"Recall: %.2f%%\" % (Recall * 100.0))\n",
    "print(\"Prec: %.2f%%\" % (Prec * 100.0))\n",
    "print(\"F1: %.2f%%\" % (f1 * 100.0))\n",
    "print(classification_report(y_train1, predicted, digits=3))\n",
    "\n",
    "c_matrix(y_train1, predicted)\n",
    "\n",
    "ROC(y_train1, y_train_probs4)\n",
    "fprmacro_train4=fprmacro\n",
    "tprmacro_train4=tprmacro\n",
    "CI_macro_train4=[0,0]\n",
    "CI_macro_train4[0]=CI_train4[0]\n",
    "CI_macro_train4[1]=CI_train4[1]\n",
    "\n",
    "predicted = model_LR.predict(X_test1)  # 测试样本预测\n",
    "y_test_probs4 = model_LR.predict_proba(X_test1)\n",
    "\n",
    "# 输出测试集预测评价指标结果\n",
    "auc_score4 = metrics.roc_auc_score(y_test1, y_test_probs4, average='macro', multi_class='ovr')  # auc roc曲线下面积\n",
    "accuracy = metrics.accuracy_score(y_test1, predicted)  # accuracy准确率\n",
    "Recall = metrics.recall_score(y_test1, predicted, average='weighted')   # recall 召回率\n",
    "Prec = metrics.precision_score(y_test1, predicted, average='weighted')  #precision 精度\n",
    "f1 = metrics.f1_score(y_test1, predicted, average='weighted') # f1 score f1分数\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "print(\"auc: %.2f%%\" % (auc_score4 * 100.0))\n",
    "print(\"Recall: %.2f%%\" % (Recall * 100.0))\n",
    "print(\"Prec: %.2f%%\" % (Prec * 100.0))\n",
    "print(\"F1: %.2f%%\" % (f1 * 100.0))\n",
    "print(classification_report(y_test1, predicted, digits=3))\n",
    "\n",
    "c_matrix(y_test1, predicted)\n",
    "\n",
    "ROC(y_test1, y_test_probs4)\n",
    "fprmacro_test4=fprmacro\n",
    "tprmacro_test4=tprmacro\n",
    "CI_macro_test4=[0,0]\n",
    "CI_macro_test4[0]=CI_train4[0]\n",
    "CI_macro_test4[1]=CI_train4[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_SVM = OneVsRestClassifier(svm.SVC(kernel='rbf',gamma = 'scale',probability=True, random_state=100))\n",
    "model_SVM.fit(X_train1,y_train1)\n",
    "\n",
    "predicted = model_SVM.predict(X_train1)\n",
    "y_train_probs5 = model_SVM.predict_proba(X_train1)\n",
    "auc5 = metrics.roc_auc_score(y_train1, y_train_probs5, average='macro', multi_class='ovr')  # auc roc曲线下面积\n",
    "accuracy = metrics.accuracy_score(y_train1, predicted)  # accuracy准确率\n",
    "Recall = metrics.recall_score(y_train1, predicted, average='weighted')   # recall 召回率\n",
    "Prec = metrics.precision_score(y_train1, predicted, average='weighted')  #precision 精度\n",
    "f1 = metrics.f1_score(y_train1, predicted, average='weighted') # f1 score f1分数\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "print(\"auc: %.2f%%\" % (auc5 * 100.0))\n",
    "print(\"Recall: %.2f%%\" % (Recall * 100.0))\n",
    "print(\"Prec: %.2f%%\" % (Prec * 100.0))\n",
    "print(\"F1: %.2f%%\" % (f1 * 100.0))\n",
    "print(classification_report(y_train1, predicted, digits=3))\n",
    "\n",
    "c_matrix(y_train1, predicted)\n",
    "\n",
    "ROC(y_train1, y_train_probs5)\n",
    "fprmacro_train5=fprmacro\n",
    "tprmacro_train5=tprmacro\n",
    "CI_macro_train5=[0,0]\n",
    "CI_macro_train5[0]=CI_train4[0]\n",
    "CI_macro_train5[1]=CI_train4[1]\n",
    "\n",
    "predicted = model_SVM.predict(X_test1)  # 测试样本预测\n",
    "y_test_probs5 = model_SVM.predict_proba(X_test1)\n",
    "\n",
    "# 输出测试集预测评价指标结果\n",
    "auc_score5 = metrics.roc_auc_score(y_test1, y_test_probs5, average='macro', multi_class='ovr')  # auc roc曲线下面积\n",
    "accuracy = metrics.accuracy_score(y_test1, predicted)  # accuracy准确率\n",
    "Recall = metrics.recall_score(y_test1, predicted, average='weighted')   # recall 召回率\n",
    "Prec = metrics.precision_score(y_test1, predicted, average='weighted')  #precision 精度\n",
    "f1 = metrics.f1_score(y_test1, predicted, average='weighted') # f1 score f1分数\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "print(\"auc: %.2f%%\" % (auc_score5 * 100.0))\n",
    "print(\"Recall: %.2f%%\" % (Recall * 100.0))\n",
    "print(\"Prec: %.2f%%\" % (Prec * 100.0))\n",
    "print(\"F1: %.2f%%\" % (f1 * 100.0))\n",
    "print(classification_report(y_test1, predicted, digits=3))\n",
    "\n",
    "c_matrix(y_test1, predicted)\n",
    "\n",
    "ROC(y_test1, y_test_probs5)\n",
    "fprmacro_test5=fprmacro\n",
    "tprmacro_test5=tprmacro\n",
    "CI_macro_test5=[0,0]\n",
    "CI_macro_test5[0]=CI_train4[0]\n",
    "CI_macro_test5[1]=CI_train4[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_RF = OneVsRestClassifier(RandomForestClassifier(n_estimators = 100, random_state = 100, class_weight = 'balanced', criterion = 'entropy'))\n",
    "model_RF.fit(X_train1,y_train1)\n",
    "\n",
    "predicted = model_RF.predict(X_train1)\n",
    "y_train_probs6 = model_RF.predict_proba(X_train1)\n",
    "auc6 = metrics.roc_auc_score(y_train1, y_train_probs6, average='macro', multi_class='ovr')  # auc roc曲线下面积\n",
    "accuracy = metrics.accuracy_score(y_train1, predicted)  # accuracy准确率\n",
    "Recall = metrics.recall_score(y_train1, predicted, average='weighted')   # recall 召回率\n",
    "Prec = metrics.precision_score(y_train1, predicted, average='weighted')  #precision 精度\n",
    "f1 = metrics.f1_score(y_train1, predicted, average='weighted') # f1 score f1分数\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "print(\"auc: %.2f%%\" % (auc6 * 100.0))\n",
    "print(\"Recall: %.2f%%\" % (Recall * 100.0))\n",
    "print(\"Prec: %.2f%%\" % (Prec * 100.0))\n",
    "print(\"F1: %.2f%%\" % (f1 * 100.0))\n",
    "print(classification_report(y_train1, predicted, digits=3))\n",
    "\n",
    "c_matrix(y_train1, predicted)\n",
    "\n",
    "ROC(y_train1, y_train_probs6)\n",
    "fprmacro_train6=fprmacro\n",
    "tprmacro_train6=tprmacro\n",
    "CI_macro_train6=[0,0]\n",
    "CI_macro_train6[0]=CI_train4[0]\n",
    "CI_macro_train6[1]=CI_train4[1]\n",
    "\n",
    "predicted = model_RF.predict(X_test1)  # 测试样本预测\n",
    "y_test_probs6 = model_RF.predict_proba(X_test1)\n",
    "\n",
    "# 输出测试集预测评价指标结果\n",
    "auc_score6 = metrics.roc_auc_score(y_test1, y_test_probs6, average='macro', multi_class='ovr')  # auc roc曲线下面积\n",
    "accuracy = metrics.accuracy_score(y_test1, predicted)  # accuracy准确率\n",
    "Recall = metrics.recall_score(y_test1, predicted, average='weighted')   # recall 召回率\n",
    "Prec = metrics.precision_score(y_test1, predicted, average='weighted')  #precision 精度\n",
    "f1 = metrics.f1_score(y_test1, predicted, average='weighted') # f1 score f1分数\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "print(\"auc: %.2f%%\" % (auc_score6 * 100.0))\n",
    "print(\"Recall: %.2f%%\" % (Recall * 100.0))\n",
    "print(\"Prec: %.2f%%\" % (Prec * 100.0))\n",
    "print(\"F1: %.2f%%\" % (f1 * 100.0))\n",
    "print(classification_report(y_test1, predicted, digits=3))\n",
    "\n",
    "c_matrix(y_test1, predicted)\n",
    "\n",
    "ROC(y_test1, y_test_probs6)\n",
    "fprmacro_test6=fprmacro\n",
    "tprmacro_test6=tprmacro\n",
    "CI_macro_test6=[0,0]\n",
    "CI_macro_test6[0]=CI_train4[0]\n",
    "CI_macro_test6[1]=CI_train4[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ADA = OneVsRestClassifier(AdaBoostClassifier(n_estimators=50, random_state = 100))\n",
    "model_ADA.fit(X_train1,y_train1)\n",
    "\n",
    "predicted = model_ADA.predict(X_train1)\n",
    "y_train_probs7 = model_ADA.predict_proba(X_train1)\n",
    "auc7 = metrics.roc_auc_score(y_train1, y_train_probs7, average='macro', multi_class='ovr')  # auc roc曲线下面积\n",
    "accuracy = metrics.accuracy_score(y_train1, predicted)  # accuracy准确率\n",
    "Recall = metrics.recall_score(y_train1, predicted, average='weighted')   # recall 召回率\n",
    "Prec = metrics.precision_score(y_train1, predicted, average='weighted')  #precision 精度\n",
    "f1 = metrics.f1_score(y_train1, predicted, average='weighted') # f1 score f1分数\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "print(\"auc: %.2f%%\" % (auc7 * 100.0))\n",
    "print(\"Recall: %.2f%%\" % (Recall * 100.0))\n",
    "print(\"Prec: %.2f%%\" % (Prec * 100.0))\n",
    "print(\"F1: %.2f%%\" % (f1 * 100.0))\n",
    "print(classification_report(y_train1, predicted, digits=3))\n",
    "\n",
    "c_matrix(y_train1, predicted)\n",
    "\n",
    "ROC(y_train1, y_train_probs7)\n",
    "fprmacro_train7=fprmacro\n",
    "tprmacro_train7=tprmacro\n",
    "CI_macro_train7=[0,0]\n",
    "CI_macro_train7[0]=CI_train4[0]\n",
    "CI_macro_train7[1]=CI_train4[1]\n",
    "\n",
    "predicted = model_ADA.predict(X_test1)  # 测试样本预测\n",
    "y_test_probs7 = model_ADA.predict_proba(X_test1)\n",
    "\n",
    "# 输出测试集预测评价指标结果\n",
    "auc_score7 = metrics.roc_auc_score(y_test1, y_test_probs7, average='macro', multi_class='ovr')  # auc roc曲线下面积\n",
    "accuracy = metrics.accuracy_score(y_test1, predicted)  # accuracy准确率\n",
    "Recall = metrics.recall_score(y_test1, predicted, average='weighted')   # recall 召回率\n",
    "Prec = metrics.precision_score(y_test1, predicted, average='weighted')  #precision 精度\n",
    "f1 = metrics.f1_score(y_test1, predicted, average='weighted') # f1 score f1分数\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "print(\"auc: %.2f%%\" % (auc_score7 * 100.0))\n",
    "print(\"Recall: %.2f%%\" % (Recall * 100.0))\n",
    "print(\"Prec: %.2f%%\" % (Prec * 100.0))\n",
    "print(\"F1: %.2f%%\" % (f1 * 100.0))\n",
    "print(classification_report(y_test1, predicted, digits=3))\n",
    "\n",
    "c_matrix(y_test1, predicted)\n",
    "\n",
    "ROC(y_test1, y_test_probs7)\n",
    "fprmacro_test7=fprmacro\n",
    "tprmacro_test7=tprmacro\n",
    "CI_macro_test7=[0,0]\n",
    "CI_macro_test7[0]=CI_train4[0]\n",
    "CI_macro_test7[1]=CI_train4[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_GBDT = OneVsRestClassifier(GradientBoostingClassifier(n_estimators=50, learning_rate=1.0,max_depth=1, random_state=100))\n",
    "model_GBDT.fit(X_train1,y_train1)\n",
    "\n",
    "predicted = model_GBDT.predict(X_train1)\n",
    "y_train_probs8 = model_GBDT.predict_proba(X_train1)\n",
    "auc8 = metrics.roc_auc_score(y_train1, y_train_probs8, average='macro', multi_class='ovr')  # auc roc曲线下面积\n",
    "accuracy = metrics.accuracy_score(y_train1, predicted)  # accuracy准确率\n",
    "Recall = metrics.recall_score(y_train1, predicted, average='weighted')   # recall 召回率\n",
    "Prec = metrics.precision_score(y_train1, predicted, average='weighted')  #precision 精度\n",
    "f1 = metrics.f1_score(y_train1, predicted, average='weighted') # f1 score f1分数\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "print(\"auc: %.2f%%\" % (auc8 * 100.0))\n",
    "print(\"Recall: %.2f%%\" % (Recall * 100.0))\n",
    "print(\"Prec: %.2f%%\" % (Prec * 100.0))\n",
    "print(\"F1: %.2f%%\" % (f1 * 100.0))\n",
    "print(classification_report(y_train1, predicted, digits=3))\n",
    "\n",
    "c_matrix(y_train1, predicted)\n",
    "\n",
    "ROC(y_train1, y_train_probs8)\n",
    "fprmacro_train8=fprmacro\n",
    "tprmacro_train8=tprmacro\n",
    "CI_macro_train8=[0,0]\n",
    "CI_macro_train8[0]=CI_train4[0]\n",
    "CI_macro_train8[1]=CI_train4[1]\n",
    "\n",
    "predicted = model_GBDT.predict(X_test1)  # 测试样本预测\n",
    "y_test_probs8 = model_GBDT.predict_proba(X_test1)\n",
    "\n",
    "# 输出测试集预测评价指标结果\n",
    "auc_score8 = metrics.roc_auc_score(y_test1, y_test_probs8, average='macro', multi_class='ovr')  # auc roc曲线下面积\n",
    "accuracy = metrics.accuracy_score(y_test1, predicted)  # accuracy准确率\n",
    "Recall = metrics.recall_score(y_test1, predicted, average='weighted')   # recall 召回率\n",
    "Prec = metrics.precision_score(y_test1, predicted, average='weighted')  #precision 精度\n",
    "f1 = metrics.f1_score(y_test1, predicted, average='weighted') # f1 score f1分数\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "print(\"auc: %.2f%%\" % (auc_score8 * 100.0))\n",
    "print(\"Recall: %.2f%%\" % (Recall * 100.0))\n",
    "print(\"Prec: %.2f%%\" % (Prec * 100.0))\n",
    "print(\"F1: %.2f%%\" % (f1 * 100.0))\n",
    "print(classification_report(y_test1, predicted, digits=3))\n",
    "\n",
    "c_matrix(y_test1, predicted)\n",
    "\n",
    "ROC(y_test1, y_test_probs8)\n",
    "fprmacro_test8=fprmacro\n",
    "tprmacro_test8=tprmacro\n",
    "CI_macro_test8=[0,0]\n",
    "CI_macro_test8[0]=CI_train4[0]\n",
    "CI_macro_test8[1]=CI_train4[1]\n",
    "\n",
    "dataframe = pd.DataFrame({'label':y_train1,'pred1':y_train_probs8[:,0],'pred2':y_train_probs8[:,1],'pred3':y_train_probs8[:,2]})\n",
    "dataframe.to_csv(\"train-CT-三分类.csv\",index=False,sep=',')\n",
    "dataframe = pd.DataFrame({'label':y_test1,'pred1':y_test_probs8[:,0],'pred2':y_test_probs8[:,1],'pred3':y_test_probs8[:,2]})\n",
    "dataframe.to_csv(\"test-CT-三分类.csv\",index=False,sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_XGB = xgb.XGBClassifier()\n",
    "params = {\n",
    "    'max_depth': 6,# 构建树的深度，越大越容易过拟合\n",
    "    'learning_rate': 0.3,# 如同学习率  \n",
    "    'n_estimators': 100, #树的个数\n",
    "    'silent': 0,#设置成1则没有运行信息输出，最好是设置为0.是否在运行升级时打印消息\n",
    "    'objective': 'multi:softprob',#多分类的问题 指定学习任务和相应的学习目标\n",
    "    'num_class': 3, # 类别数，多分类与 multisoftmax 并用\n",
    "    'nthread': 4,# cpu 线程数 默认最大\n",
    "    'gamma': 0,# 树的叶子节点上作进一步分区所需的最小损失减少,越大越保守，一般0.1、0.2这样子。\n",
    "    'subsample': 1,# 随机采样训练样本 训练实例的子采样比\n",
    "    'min_child_weight': 1, \n",
    "                          # 这个参数默认是 1，是每个叶子里面 h 的和至少是多少，对正负样本不均衡时的 0-1 分类而言\n",
    "                          #，假设 h 在 0.01 附近，min_child_weight 为 1 意味着叶子节点中最少需要包含 100 个样本。\n",
    "                          #这个参数非常影响结果，控制叶子节点中二阶导的和的最小值，该参数值越小，越容易 overfitting\n",
    "    'colsample_bytree': 1,\n",
    "    'max_delta_step': 0,#最大增量步长，我们允许每个树的权重估计。\n",
    "    'reg_lambda': 1,  # 控制模型复杂度的权重值的L2正则化项参数，参数越大，模型越不容易过拟合。\n",
    "    'seed': 100,#随机种子\n",
    "    'reg_alpha': 0, # L1 正则项参数\n",
    "    #'scale_pos_weight': 1, #如果取值大于0的话，在类别样本不平衡的情况下有助于快速收敛。平衡正负权重\n",
    "    'eval_metric': 'auc'\n",
    "}\n",
    "model_XGB.fit(X_train1, y_train1)\n",
    "\n",
    "predicted = model_XGB.predict(X_train1)\n",
    "y_train_probs1 = model_XGB.predict_proba(X_train1)\n",
    "auc1 = metrics.roc_auc_score(y_train1, y_train_probs1, average='macro', multi_class='ovr')  # auc roc曲线下面积\n",
    "accuracy = metrics.accuracy_score(y_train1, predicted)  # accuracy准确率\n",
    "Recall = metrics.recall_score(y_train1, predicted, average='weighted')   # recall 召回率\n",
    "Prec = metrics.precision_score(y_train1, predicted, average='weighted')  #precision 精度\n",
    "f1 = metrics.f1_score(y_train1, predicted, average='weighted') # f1 score f1分数\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "print(\"auc: %.2f%%\" % (auc1 * 100.0))\n",
    "print(\"Recall: %.2f%%\" % (Recall * 100.0))\n",
    "print(\"Prec: %.2f%%\" % (Prec * 100.0))\n",
    "print(\"F1: %.2f%%\" % (f1 * 100.0))\n",
    "print(classification_report(y_train1, predicted, digits=3))\n",
    "\n",
    "c_matrix(y_train1, predicted)\n",
    "\n",
    "ROC(y_train1, y_train_probs1)\n",
    "fprmacro_train1=fprmacro\n",
    "tprmacro_train1=tprmacro\n",
    "CI_macro_train1=[0,0]\n",
    "CI_macro_train1[0]=CI_train4[0]\n",
    "CI_macro_train1[1]=CI_train4[1]\n",
    "\n",
    "predicted = model_XGB.predict(X_test1)  # 测试样本预测\n",
    "y_test_probs1 = model_XGB.predict_proba(X_test1)\n",
    "\n",
    "# 输出测试集预测评价指标结果\n",
    "auc_score1 = metrics.roc_auc_score(y_test1, y_test_probs1, average='macro', multi_class='ovr')  # auc roc曲线下面积\n",
    "accuracy = metrics.accuracy_score(y_test1, predicted)  # accuracy准确率\n",
    "Recall = metrics.recall_score(y_test1, predicted, average='weighted')   # recall 召回率\n",
    "Prec = metrics.precision_score(y_test1, predicted, average='weighted')  #precision 精度\n",
    "f1 = metrics.f1_score(y_test1, predicted, average='weighted') # f1 score f1分数\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "print(\"auc: %.2f%%\" % (auc_score1 * 100.0))\n",
    "print(\"Recall: %.2f%%\" % (Recall * 100.0))\n",
    "print(\"Prec: %.2f%%\" % (Prec * 100.0))\n",
    "print(\"F1: %.2f%%\" % (f1 * 100.0))\n",
    "print(classification_report(y_test1, predicted, digits=3))\n",
    "\n",
    "c_matrix(y_test1, predicted)\n",
    "\n",
    "ROC(y_test1, y_test_probs1)\n",
    "fprmacro_test1=fprmacro\n",
    "tprmacro_test1=tprmacro\n",
    "CI_macro_test1=[0,0]\n",
    "CI_macro_test1[0]=CI_train4[0]\n",
    "CI_macro_test1[1]=CI_train4[1]\n",
    "\n",
    "# 显示重要特征\n",
    "plot_importance(model_XGB)\n",
    "plt.show()\n",
    "\n",
    "# 打印测试集RMSE\n",
    "rmse = sqrt(mean_squared_error(np.array(list(y_test1)), np.array(list(predicted))))\n",
    "print(\"rmse:\",rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=1.2)\n",
    "plt.style.use('seaborn-white')\n",
    "plt.figure(dpi=300)\n",
    "\n",
    "plt.plot(fprmacro_train4,tprmacro_train4,'lime', label='{}= {} ({}-{})'.format('LR_macro_AUC', '%.3f' % auc4, '%.3f' % CI_macro_train4[0], '%.3f' % CI_macro_train4[1]))\n",
    "plt.plot(fprmacro_train5,tprmacro_train5,'magenta', label='{}= {} ({}-{})'.format('SVM_macro_AUC', '%.3f' % auc5, '%.3f' % CI_macro_train5[0], '%.3f' % CI_macro_train5[1]))\n",
    "plt.plot(fprmacro_train6,tprmacro_train6,'purple', label='{}= {} ({}-{})'.format('RF_macro_AUC', '%.3f' % auc6, '%.3f' % CI_macro_train6[0], '%.3f' % CI_macro_train6[1]))\n",
    "plt.plot(fprmacro_train7,tprmacro_train7,'coral', label='{}= {} ({}-{})'.format('ADA_macro_AUC', '%.3f' % auc7, '%.3f' % CI_macro_train7[0], '%.3f' % CI_macro_train7[1]))\n",
    "plt.plot(fprmacro_train8,tprmacro_train8,'yellow', label='{}= {} ({}-{})'.format('GBDT_macro_AUC', '%.3f' % auc8, '%.3f' % CI_macro_train8[0], '%.3f' % CI_macro_train8[1]))\n",
    "plt.plot(fprmacro_train1,tprmacro_train1,'blue', label='{}= {} ({}-{})'.format('XGB_macro_AUC', '%.3f' % auc1, '%.3f' % CI_macro_train1[0], '%.3f' % CI_macro_train1[1]))\n",
    "\n",
    "plt.legend(loc='lower right',fontsize = 10)\n",
    "plt.plot([0,1],[0,1],'k--')\n",
    "plt.ylabel('Sensitivity',fontsize = 14)\n",
    "plt.xlabel('1 - Specificity',fontsize = 14)\n",
    "plt.savefig(\"C:/Users/Ding/Desktop/Figures and tables/三分类CT1.svg\", dpi=300,format=\"svg\",bbox_inches = 'tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=1.2)\n",
    "plt.style.use('seaborn-white')\n",
    "plt.figure(dpi=300)\n",
    "\n",
    "plt.plot(fprmacro_test4,tprmacro_test4,'lime', label='{}= {} ({}-{})'.format('LR_macro_AUC', '%.3f' % auc_score4, '%.3f' % CI_macro_test4[0], '%.3f' % CI_macro_test4[1]))\n",
    "plt.plot(fprmacro_test5,tprmacro_test5,'magenta', label='{}= {} ({}-{})'.format('SVM_macro_AUC', '%.3f' % auc_score5, '%.3f' % CI_macro_test5[0], '%.3f' % CI_macro_test5[1]))\n",
    "plt.plot(fprmacro_test6,tprmacro_test6,'purple', label='{}= {} ({}-{})'.format('RF_macro_AUC', '%.3f' % auc_score6, '%.3f' % CI_macro_test6[0], '%.3f' % CI_macro_test6[1]))\n",
    "plt.plot(fprmacro_test7,tprmacro_test7,'coral', label='{}= {} ({}-{})'.format('ADA_macro_AUC', '%.3f' % auc_score7, '%.3f' % CI_macro_test7[0], '%.3f' % CI_macro_test7[1]))\n",
    "plt.plot(fprmacro_test8,tprmacro_test8,'yellow', label='{}= {} ({}-{})'.format('GBDT_macro_AUC', '%.3f' % auc_score8, '%.3f' % CI_macro_test8[0], '%.3f' % CI_macro_test8[1]))\n",
    "plt.plot(fprmacro_test1,tprmacro_test1,'blue', label='{}= {} ({}-{})'.format('XGB_macro_AUC', '%.3f' % auc_score1, '%.3f' % CI_macro_test1[0], '%.3f' % CI_macro_test1[1]))\n",
    "\n",
    "plt.legend(loc='lower right',fontsize = 10)\n",
    "plt.plot([0,1],[0,1],'k--')\n",
    "plt.ylabel('Sensitivity',fontsize = 14)\n",
    "plt.xlabel('1 - Specificity',fontsize = 14)\n",
    "plt.savefig(\"C:/Users/Ding/Desktop/Figures and tables/三分类CT2.svg\", dpi=300,format=\"svg\",bbox_inches = 'tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#坐标轴负号的处理\n",
    "plt.rcParams['axes.unicode_minus']=False\n",
    "# 读取数据\n",
    "tips = pd.read_csv('C:/Users/Ding/Desktop/融合/三分类融合/训练集miRNA.csv')\n",
    "# 绘制分组小提琴图\n",
    "sns.boxplot(x = \"label\", # 指定x轴的数据\n",
    "               y = \"has-miR371a-3P\", # 指定y轴的数据\n",
    "               data = tips, # 指定绘图的数据集\n",
    "               order = ['Necrosis/fibrosis','Teratoma','Viable GCT'], # 指定x轴刻度标签的顺序\n",
    "               palette = 'Set2', # 指定不同性别对应的颜色（因为hue参数为设置为性别变量）\n",
    "               saturation=1, #饱和度\n",
    "               whis=1,\n",
    "               #flierprops = {'marker': 'D'},  # 异常值形状 'markerfacecolor': 'red',  # 形状填充色\n",
    "               fliersize=5, \n",
    "               whiskerprops={'linestyle':'-', 'color':'black'},  # 设置上下须属性\n",
    "               showmeans=True,  # 箱图显示均值，\n",
    "               meanprops={'marker': 's','markerfacecolor':'red', 'markeredgecolor':'red','fillstyle':'none','markersize': 5}  # 设置均值属性\n",
    "              )\n",
    "\n",
    "plt.xticks(ha = 'center',va = 'top',size=15)\n",
    "plt.yscale(\"log\")\n",
    "plt.ylim(0.005,8000)\n",
    "plt.ylabel('miR371a-3P relative expression',size=15)\n",
    "plt.xlabel('')\n",
    "x1, x2, x3= 0, 1, 2\n",
    "y,h = tips[\"has-miR371a-3P\"].mean()+1600,1000\n",
    "#绘制横线位置\n",
    "plt.plot([x2+0.05, x2+0.05, x3, x3], [y-1100, y-900, y-900, y-1100], lw=1, c=\"k\") \n",
    "plt.plot([x1, x1, x2-0.05, x2-0.05], [y-1100, y-900, y-900, y-1100], lw=1, c=\"k\")\n",
    "plt.plot([x1, x1, x3, x3], [y, y+h, y+h, y], lw=1, c=\"k\") \n",
    "#添加P值\n",
    "plt.text((x2+x3)*.5+0.025, y-1200, \"*\", ha='center', va='bottom', color=\"k\", size=24)\n",
    "plt.text((x1+x2)*.5-0.025, y-850, \"ns\", ha='center', va='bottom', color=\"k\", size=15)\n",
    "plt.text((x1+x3)*.5, y, \"*\", ha='center', va='bottom', color=\"k\", size=24)\n",
    "# sns.swarmplot(x='group1', y='rad',data=tips,color='w', alpha=0.8, size=3)\n",
    "# 显示图形\n",
    "plt.savefig(\"C:/Users/Ding/Desktop/Figures and tables/miRNA对比371-1.svg\", dpi=300,format=\"svg\",bbox_inches = 'tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#坐标轴负号的处理\n",
    "plt.rcParams['axes.unicode_minus']=False\n",
    "# 读取数据\n",
    "tips = pd.read_csv('C:/Users/Ding/Desktop/融合/三分类融合/训练集miRNA.csv')\n",
    "# 绘制分组小提琴图\n",
    "sns.boxplot(x = \"label\", # 指定x轴的数据\n",
    "               y = \"has-miR-375-5P\", # 指定y轴的数据\n",
    "               data = tips, # 指定绘图的数据集\n",
    "               order = ['Necrosis/fibrosis','Teratoma','Viable GCT'], # 指定x轴刻度标签的顺序\n",
    "               palette = 'Set2', # 指定不同性别对应的颜色（因为hue参数为设置为性别变量）\n",
    "               saturation=1, #饱和度\n",
    "               whis=5,\n",
    "               #flierprops = {'marker': 'D'},  # 异常值形状 'markerfacecolor': 'red',  # 形状填充色\n",
    "               fliersize=5, \n",
    "               whiskerprops={'linestyle':'-', 'color':'black'},  # 设置上下须属性\n",
    "               showmeans=True,  # 箱图显示均值，\n",
    "               meanprops={'marker': 's','markerfacecolor':'red', 'markeredgecolor':'red','fillstyle':'none','markersize': 5}  # 设置均值属性\n",
    "              )\n",
    "\n",
    "plt.xticks(ha = 'center',va = 'top',size=15)\n",
    "plt.yscale(\"log\")\n",
    "plt.ylim(0.005,8000)\n",
    "plt.ylabel('miR375-5P relative expression',size=15)\n",
    "plt.xlabel('')\n",
    "x1, x2, x3= 0, 1, 2\n",
    "y,h = tips[\"has-miR-375-5P\"].mean()+1600,1000\n",
    "#绘制横线位置\n",
    "plt.plot([x2+0.05, x2+0.05, x3, x3], [y-1100, y-900, y-900, y-1100], lw=1, c=\"k\") \n",
    "plt.plot([x1, x1, x2-0.05, x2-0.05], [y-1100, y-900, y-900, y-1100], lw=1, c=\"k\")\n",
    "plt.plot([x1, x1, x3, x3], [y, y+h, y+h, y], lw=1, c=\"k\") \n",
    "#添加P值\n",
    "plt.text((x2+x3)*.5+0.025, y-1200, \"*\", ha='center', va='bottom', color=\"k\", size=24)\n",
    "plt.text((x1+x2)*.5-0.025, y-1200, \"*\", ha='center', va='bottom', color=\"k\", size=24)\n",
    "plt.text((x1+x3)*.5, y+1000, \"ns\", ha='center', va='bottom', color=\"k\", size=15)\n",
    "# sns.swarmplot(x='group1', y='rad',data=tips,color='w', alpha=0.8, size=3)\n",
    "# 显示图形\n",
    "plt.savefig(\"C:/Users/Ding/Desktop/Figures and tables/miRNA对比375-1.svg\", dpi=300,format=\"svg\",bbox_inches = 'tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_LR = OneVsRestClassifier(LogisticRegression(random_state = 100))\n",
    "model_LR.fit(X_train2,y_train2)\n",
    "\n",
    "predicted = model_LR.predict(X_train2)\n",
    "y_train_probs9 = model_LR.predict_proba(X_train2)\n",
    "auc9 = metrics.roc_auc_score(y_train2, y_train_probs9, average='macro', multi_class='ovr')  # auc roc曲线下面积\n",
    "accuracy = metrics.accuracy_score(y_train2, predicted)  # accuracy准确率\n",
    "Recall = metrics.recall_score(y_train2, predicted, average='weighted')   # recall 召回率\n",
    "Prec = metrics.precision_score(y_train2, predicted, average='weighted')  #precision 精度\n",
    "f1 = metrics.f1_score(y_train2, predicted, average='weighted') # f1 score f1分数\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "print(\"auc: %.2f%%\" % (auc9 * 100.0))\n",
    "print(\"Recall: %.2f%%\" % (Recall * 100.0))\n",
    "print(\"Prec: %.2f%%\" % (Prec * 100.0))\n",
    "print(\"F1: %.2f%%\" % (f1 * 100.0))\n",
    "print(classification_report(y_train2, predicted, digits=3))\n",
    "\n",
    "c_matrix(y_train2, predicted)\n",
    "\n",
    "ROC(y_train2, y_train_probs9)\n",
    "fprmacro_train9=fprmacro\n",
    "tprmacro_train9=tprmacro\n",
    "CI_macro_train9=[0,0]\n",
    "CI_macro_train9[0]=CI_train4[0]\n",
    "CI_macro_train9[1]=CI_train4[1]\n",
    "\n",
    "predicted = model_LR.predict(X_test2)  # 测试样本预测\n",
    "y_test_probs9 = model_LR.predict_proba(X_test2)\n",
    "\n",
    "# 输出测试集预测评价指标结果\n",
    "auc_score9 = metrics.roc_auc_score(y_test2, y_test_probs9, average='macro', multi_class='ovr')  # auc roc曲线下面积\n",
    "accuracy = metrics.accuracy_score(y_test2, predicted)  # accuracy准确率\n",
    "Recall = metrics.recall_score(y_test2, predicted, average='weighted')   # recall 召回率\n",
    "Prec = metrics.precision_score(y_test2, predicted, average='weighted')  #precision 精度\n",
    "f1 = metrics.f1_score(y_test2, predicted, average='weighted') # f1 score f1分数\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "print(\"auc: %.2f%%\" % (auc_score9 * 100.0))\n",
    "print(\"Recall: %.2f%%\" % (Recall * 100.0))\n",
    "print(\"Prec: %.2f%%\" % (Prec * 100.0))\n",
    "print(\"F1: %.2f%%\" % (f1 * 100.0))\n",
    "print(classification_report(y_test2, predicted, digits=3))\n",
    "\n",
    "c_matrix(y_test2, predicted)\n",
    "\n",
    "ROC(y_test2, y_test_probs9)\n",
    "fprmacro_test9=fprmacro\n",
    "tprmacro_test9=tprmacro\n",
    "CI_macro_test9=[0,0]\n",
    "CI_macro_test9[0]=CI_train4[0]\n",
    "CI_macro_test9[1]=CI_train4[1]\n",
    "dataframe = pd.DataFrame({'label':y_train2,'pred1':y_train_probs9[:,0],'pred2':y_train_probs9[:,1],'pred3':y_train_probs9[:,2]})\n",
    "dataframe.to_csv(\"train-RNA-三分类.csv\",index=False,sep=',')\n",
    "dataframe = pd.DataFrame({'label':y_test2,'pred1':y_test_probs9[:,0],'pred2':y_test_probs9[:,1],'pred3':y_test_probs9[:,2]})\n",
    "dataframe.to_csv(\"test-RNA-三分类.csv\",index=False,sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlsx_a = 'C:/Users/Ding/Desktop/融合/三分类融合/融合train结果.xlsx'\n",
    "xlsx_b = 'C:/Users/Ding/Desktop/融合/三分类融合/融合test结果.xlsx'\n",
    "data_a = pd.read_excel(xlsx_a)\n",
    "data_b = pd.read_excel(xlsx_b)\n",
    "rows_a,cols_a = data_a.shape\n",
    "rows_b,cols_b = data_b.shape\n",
    "y_train_probs3 = np.ones((rows_a,cols_a))\n",
    "y_train_probs3= data_a.iloc[:,1:4]\n",
    "y_train3 = data_a['label']\n",
    "predicted1 = data_a['predicted1']\n",
    "y_test_probs3 = np.ones((rows_b,cols_b))\n",
    "y_test_probs3= data_b.iloc[:,1:4]\n",
    "y_test3 = data_b['label']\n",
    "predicted2 = data_b['predicted1']\n",
    "y_train_probs3=y_train_probs3.values\n",
    "y_test_probs3=y_test_probs3.values\n",
    "\n",
    "y_train_probs4 = np.ones((rows_a,cols_a))\n",
    "y_train_probs4= data_a.iloc[:,5:8]\n",
    "y_train4 = data_a['label']\n",
    "predicted3 = data_a['predicted2']\n",
    "y_test_probs4 = np.ones((rows_b,cols_b))\n",
    "y_test_probs4= data_b.iloc[:,5:8]\n",
    "y_test4 = data_b['label']\n",
    "predicted4 = data_b['predicted2']\n",
    "y_train_probs4=y_train_probs4.values\n",
    "y_test_probs4=y_test_probs4.values\n",
    "\n",
    "y_train_probs5 = np.ones((rows_a,cols_a))\n",
    "y_train_probs5= data_a.iloc[:,9:12]\n",
    "y_train5 = data_a['label']\n",
    "predicted5 = data_a['predicted3']\n",
    "y_test_probs5 = np.ones((rows_b,cols_b))\n",
    "y_test_probs5= data_b.iloc[:,9:12]\n",
    "y_test5 = data_b['label']\n",
    "predicted6 = data_b['predicted3']\n",
    "y_train_probs5=y_train_probs5.values\n",
    "y_test_probs5=y_test_probs5.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=1.2)\n",
    "plt.style.use('seaborn-white')\n",
    "plt.figure(dpi=300)\n",
    "\n",
    "y_train = label_binarize(y_train5, classes=[0, 1, 2])\n",
    "fpr1, tpr1, thresholds1 = roc_curve(y_train[:,0], y_train_probs5[:,0], pos_label = 1)\n",
    "fpr2, tpr2, thresholds2 = roc_curve(y_train[:,1], y_train_probs5[:,1], pos_label = 1)\n",
    "fpr3, tpr3, thresholds3 = roc_curve(y_train[:,2], y_train_probs5[:,2], pos_label = 1)\n",
    "\n",
    "all_fpr = np.unique(np.concatenate([fpr1, fpr2, fpr3]))\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "mean_tpr += np.interp(all_fpr, fpr1, tpr1)\n",
    "mean_tpr += np.interp(all_fpr, fpr2, tpr2)\n",
    "mean_tpr += np.interp(all_fpr, fpr3, tpr3)\n",
    "mean_tpr /= 3\n",
    "fprmacro = all_fpr\n",
    "tprmacro = mean_tpr\n",
    "fprmacro = np.insert(fprmacro,0,values=[0])\n",
    "tprmacro = np.insert(tprmacro,0,values=[0])\n",
    "y1 = tpr1 - fpr1\n",
    "y2 = tpr2 - fpr2\n",
    "y3 = tpr3 - fpr3\n",
    "y4 = tprmacro - fprmacro\n",
    "Youden_index1 = np.argmax(y1)# Only the first occurrence is returned.\n",
    "Youden_index2 = np.argmax(y2)\n",
    "Youden_index3 = np.argmax(y3)\n",
    "Youden_index4 = np.argmax(y4)\n",
    "optimal_threshold = np.array([thresholds1[Youden_index1],thresholds2[Youden_index2],thresholds3[Youden_index3]])\n",
    "point = np.array([[1-fpr1[Youden_index1], tpr1[Youden_index1]],[1-fpr2[Youden_index2], tpr2[Youden_index2]],[1-fpr3[Youden_index3], tpr3[Youden_index3]],[1-fprmacro[Youden_index4], tprmacro[Youden_index4]]])\n",
    "print(optimal_threshold)\n",
    "print('Specificity and Sensitivity')\n",
    "print(point)\n",
    "\n",
    "plt.figure()\n",
    "# plt.plot(fpr, tpr, marker = 'o')\n",
    "auc1 = roc_auc_score(y_train[:,0], y_train_probs5[:,0])\n",
    "auc2 = roc_auc_score(y_train[:,1], y_train_probs5[:,1])\n",
    "auc3 = roc_auc_score(y_train[:,2], y_train_probs5[:,2])\n",
    "auc4 = metrics.roc_auc_score(y_train, y_train_probs5, average='macro', multi_class='ovr')\n",
    "CI_train1=AUC_CI(auc1, y_train[:,0], alpha = 0.05)\n",
    "CI_train2=AUC_CI(auc2, y_train[:,1], alpha = 0.05)\n",
    "CI_train3=AUC_CI(auc3, y_train[:,2], alpha = 0.05)\n",
    "CI_train4=[0,0]\n",
    "CI_train4[0]=(float(CI_train1[0])+float(CI_train2[0])+float(CI_train3[0]))/3\n",
    "CI_train4[1]=(float(CI_train1[1])+float(CI_train2[1])+float(CI_train3[1]))/3\n",
    "\n",
    "plt.plot(fpr1,tpr1,'#1f77b4', label='{}= {} ({}-{})'.format('N vs.O_AUC', '%.3f' % auc1, CI_train1[0], CI_train1[1]))\n",
    "plt.plot(fpr2,tpr2,'#ff7f0e', label='{}= {} ({}-{})'.format('T vs.O_AUC', '%.3f' % auc2, CI_train2[0], CI_train2[1]))\n",
    "plt.plot(fpr3,tpr3,'#2ca02c', label='{}= {} ({}-{})'.format('V vs.O_AUC', '%.3f' % auc3, CI_train3[0], CI_train3[1]))\n",
    "plt.plot(fprmacro, tprmacro, label='{}= {} ({}-{})'.format('macro-average ROC_AUC', '%.3f' % auc4, '%.3f' % CI_train4[0], '%.3f' % CI_train4[1]),\n",
    "         color='navy', linestyle=':', linewidth=2)\n",
    "\n",
    "plt.xlabel(\"1 - Specificity\",size='18')\n",
    "plt.ylabel(\"Sensitivity\",size='18')\n",
    "plt.xticks(fontsize=16,family='Arial')\n",
    "plt.yticks(fontsize=16,family='Arial')\n",
    "plt.legend(loc='lower right',fontsize = 10)\n",
    "plt.plot([0,1],[0,1],'k--')\n",
    "plt.savefig(\"C:/Users/Ding/Desktop/Figures and tables/三分类CT+371+375后融合1.svg\", dpi=300,format=\"svg\",bbox_inches = 'tight')\n",
    "plt.show()\n",
    "\n",
    "fprmacro_train5=fprmacro\n",
    "tprmacro_train5=tprmacro\n",
    "CI_macro_train5=[0,0]\n",
    "CI_macro_train5[0]=float(CI_train4[0])\n",
    "CI_macro_train5[1]=float(CI_train4[1])\n",
    "\n",
    "accuracy = metrics.accuracy_score(y_train5, predicted5)  # accuracy准确率\n",
    "Recall = metrics.recall_score(y_train5, predicted5, average='weighted')   # recall 召回率\n",
    "Prec = metrics.precision_score(y_train5, predicted5, average='weighted')  #precision 精度\n",
    "f1 = metrics.f1_score(y_train5, predicted5, average='weighted') # f1 score f1分数\n",
    "print(\"Accuracy: %.3f%%\" % (accuracy * 100.0))\n",
    "print(\"Recall: %.3f%%\" % (Recall * 100.0))\n",
    "print(\"Prec: %.3f%%\" % (Prec * 100.0))\n",
    "print(\"F1: %.3f%%\" % (f1 * 100.0))\n",
    "print(classification_report(y_train5, predicted5, digits=3))\n",
    "\n",
    "C = confusion_matrix(y_train5, predicted5)\n",
    "plt.matshow(C, cmap=plt.cm.Blues) # 根据最下面的图按自己需求更改颜色\n",
    "plt.colorbar()\n",
    "thresh = C.max() / 2.\n",
    "for i in range(len(C)):\n",
    "    for j in range(len(C)):\n",
    "        plt.annotate(C[j, i], xy=(i, j), horizontalalignment='center', verticalalignment='center', color=\"white\" if C[j, i] > thresh else \"black\")\n",
    "plt.tick_params(labelsize=18) # 设置左边和上面的label类别如0,1,2,3,4的字体大小。\n",
    "plt.ylabel('True label', fontdict={'family': 'Arial', 'size': 20}) # 设置字体大小\n",
    "plt.xlabel('Predicted label', fontdict={'family': 'Arial', 'size': 20})\n",
    "plt.xticks([0,1,2], labels=['N','T','V']) # 将x轴或y轴坐标，刻度 替换为文字/字符\n",
    "plt.yticks([0,1,2], labels=['N','T','V'], rotation='vertical')\n",
    "plt.savefig(\"C:/Users/Ding/Desktop/Figures and tables/三分类CT+371+375后融合混淆矩阵1.svg\", dpi=300,format=\"svg\",bbox_inches = 'tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=1.2)\n",
    "plt.style.use('seaborn-white')\n",
    "plt.figure(dpi=300)\n",
    "\n",
    "y_test = label_binarize(y_test5, classes=[0, 1, 2])\n",
    "fpr1, tpr1, thresholds1 = roc_curve(y_test[:,0], y_test_probs5[:,0], pos_label = 1)\n",
    "fpr2, tpr2, thresholds2 = roc_curve(y_test[:,1], y_test_probs5[:,1], pos_label = 1)\n",
    "fpr3, tpr3, thresholds3 = roc_curve(y_test[:,2], y_test_probs5[:,2], pos_label = 1)\n",
    "\n",
    "all_fpr = np.unique(np.concatenate([fpr1, fpr2, fpr3]))\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "mean_tpr += np.interp(all_fpr, fpr1, tpr1)\n",
    "mean_tpr += np.interp(all_fpr, fpr2, tpr2)\n",
    "mean_tpr += np.interp(all_fpr, fpr3, tpr3)\n",
    "mean_tpr /= 3\n",
    "fprmacro = all_fpr\n",
    "tprmacro = mean_tpr\n",
    "fprmacro = np.insert(fprmacro,0,values=[0])\n",
    "tprmacro = np.insert(tprmacro,0,values=[0])\n",
    "y1 = tpr1 - fpr1\n",
    "y2 = tpr2 - fpr2\n",
    "y3 = tpr3 - fpr3\n",
    "y4 = tprmacro - fprmacro\n",
    "Youden_index1 = np.argmax(y1)# Only the first occurrence is returned.\n",
    "Youden_index2 = np.argmax(y2)\n",
    "Youden_index3 = np.argmax(y3)\n",
    "Youden_index4 = np.argmax(y4)\n",
    "optimal_threshold = np.array([thresholds1[Youden_index1],thresholds2[Youden_index2],thresholds3[Youden_index3]])\n",
    "point = np.array([[1-fpr1[Youden_index1], tpr1[Youden_index1]],[1-fpr2[Youden_index2], tpr2[Youden_index2]],[1-fpr3[Youden_index3], tpr3[Youden_index3]],[1-fprmacro[Youden_index4], tprmacro[Youden_index4]]])\n",
    "print(optimal_threshold)\n",
    "print('Specificity and Sensitivity')\n",
    "print(point)\n",
    "\n",
    "plt.figure()\n",
    "# plt.plot(fpr, tpr, marker = 'o')\n",
    "auc_score1 = roc_auc_score(y_test[:,0], y_test_probs5[:,0])\n",
    "auc_score2 = roc_auc_score(y_test[:,1], y_test_probs5[:,1])\n",
    "auc_score3 = roc_auc_score(y_test[:,2], y_test_probs5[:,2])\n",
    "auc_score4 = metrics.roc_auc_score(y_test, y_test_probs5, average='macro', multi_class='ovr')\n",
    "CI_test1=AUC_CI(auc_score1, y_test[:,0], alpha = 0.05)\n",
    "CI_test2=AUC_CI(auc_score2, y_test[:,1], alpha = 0.05)\n",
    "CI_test3=AUC_CI(auc_score3, y_test[:,2], alpha = 0.05)\n",
    "CI_test4=[0,0]\n",
    "CI_test4[0]=(float(CI_test1[0])+float(CI_test2[0])+float(CI_test3[0]))/3\n",
    "CI_test4[1]=(float(CI_test1[1])+float(CI_test2[1])+float(CI_test3[1]))/3\n",
    "\n",
    "plt.plot(fpr1,tpr1,'#1f77b4', label='{}= {} ({}-{})'.format('N vs.O_AUC', '%.3f' % auc_score1, CI_test1[0], CI_test1[1]))\n",
    "plt.plot(fpr2,tpr2,'#ff7f0e', label='{}= {} ({}-{})'.format('T vs.O_AUC', '%.3f' % auc_score2, CI_test2[0], CI_test2[1]))\n",
    "plt.plot(fpr3,tpr3,'#2ca02c', label='{}= {} ({}-{})'.format('V vs.O_AUC', '%.3f' % auc_score3, CI_test3[0], CI_test3[1]))\n",
    "plt.plot(fprmacro, tprmacro, label='{}= {} ({}-{})'.format('macro-average ROC_AUC', '%.3f' % auc_score4, '%.3f' % CI_test4[0], '%.3f' % CI_test4[1]),\n",
    "         color='navy', linestyle=':', linewidth=2)\n",
    "\n",
    "plt.xlabel(\"1 - Specificity\",size='18')\n",
    "plt.ylabel(\"Sensitivity\",size='18')\n",
    "plt.xticks(fontsize=16,family='Arial')\n",
    "plt.yticks(fontsize=16,family='Arial')\n",
    "plt.legend(loc='lower right',fontsize = 10)\n",
    "plt.plot([0,1],[0,1],'k--')\n",
    "plt.savefig(\"C:/Users/Ding/Desktop/Figures and tables/三分类CT+371+375后融合2.svg\", dpi=300,format=\"svg\",bbox_inches = 'tight')\n",
    "plt.show()\n",
    "\n",
    "fprmacro_test5=fprmacro\n",
    "tprmacro_test5=tprmacro\n",
    "CI_macro_test5=[0,0]\n",
    "CI_macro_test5[0]=float(CI_test4[0])\n",
    "CI_macro_test5[1]=float(CI_test4[1])\n",
    "\n",
    "accuracy = metrics.accuracy_score(y_test5, predicted6)  # accuracy准确率\n",
    "Recall = metrics.recall_score(y_test5, predicted6, average='weighted')   # recall 召回率\n",
    "Prec = metrics.precision_score(y_test5, predicted6, average='weighted')  #precision 精度\n",
    "f1 = metrics.f1_score(y_test5, predicted6, average='weighted') # f1 score f1分数\n",
    "print(\"Accuracy: %.3f%%\" % (accuracy * 100.0))\n",
    "print(\"Recall: %.3f%%\" % (Recall * 100.0))\n",
    "print(\"Prec: %.3f%%\" % (Prec * 100.0))\n",
    "print(\"F1: %.3f%%\" % (f1 * 100.0))\n",
    "print(classification_report(y_test5, predicted6, digits=3))\n",
    "\n",
    "C = confusion_matrix(y_test5, predicted6)\n",
    "plt.matshow(C, cmap=plt.cm.Blues) # 根据最下面的图按自己需求更改颜色\n",
    "plt.colorbar()\n",
    "thresh = C.max() / 2.\n",
    "for i in range(len(C)):\n",
    "    for j in range(len(C)):\n",
    "        plt.annotate(C[j, i], xy=(i, j), horizontalalignment='center', verticalalignment='center', color=\"white\" if C[j, i] > thresh else \"black\")\n",
    "plt.tick_params(labelsize=18) # 设置左边和上面的label类别如0,1,2,3,4的字体大小。\n",
    "plt.ylabel('True label', fontdict={'family': 'Arial', 'size': 20}) # 设置字体大小\n",
    "plt.xlabel('Predicted label', fontdict={'family': 'Arial', 'size': 20})\n",
    "plt.xticks([0,1,2], labels=['N','T','V']) # 将x轴或y轴坐标，刻度 替换为文字/字符\n",
    "plt.yticks([0,1,2], labels=['N','T','V'], rotation='vertical')\n",
    "plt.savefig(\"C:/Users/Ding/Desktop/Figures and tables/三分类CT+371+375后融合混淆矩阵2.svg\", dpi=300,format=\"svg\",bbox_inches = 'tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as st\n",
    "\n",
    "class DelongTest():\n",
    "    def __init__(self,preds1,preds2,label,threshold=0.05):\n",
    "        '''\n",
    "        preds1:the output of model1\n",
    "        preds2:the output of model2\n",
    "        label :the actual label\n",
    "        '''\n",
    "        self._preds1=preds1\n",
    "        self._preds2=preds2\n",
    "        self._label=label\n",
    "        self.threshold=threshold\n",
    "        self._show_result()\n",
    "\n",
    "    def _auc(self,X, Y)->float:\n",
    "        return 1/(len(X)*len(Y)) * sum([self._kernel(x, y) for x in X for y in Y])\n",
    "\n",
    "    def _kernel(self,X, Y)->float:\n",
    "        '''\n",
    "        Mann-Whitney statistic\n",
    "        '''\n",
    "        return .5 if Y==X else int(Y < X)\n",
    "\n",
    "    def _structural_components(self,X, Y)->list:\n",
    "        V10 = [1/len(Y) * sum([self._kernel(x, y) for y in Y]) for x in X]\n",
    "        V01 = [1/len(X) * sum([self._kernel(x, y) for x in X]) for y in Y]\n",
    "        return V10, V01\n",
    "\n",
    "    def _get_S_entry(self,V_A, V_B, auc_A, auc_B)->float:\n",
    "        return 1/(len(V_A)-1) * sum([(a-auc_A)*(b-auc_B) for a,b in zip(V_A, V_B)])\n",
    "    \n",
    "    def _z_score(self,var_A, var_B, covar_AB, auc_A, auc_B):\n",
    "        return (auc_A - auc_B)/((var_A + var_B - 2*covar_AB )**(.5)+ 1e-8)\n",
    "\n",
    "    def _group_preds_by_label(self,preds, actual)->list:\n",
    "        X = [p for (p, a) in zip(preds, actual) if a]\n",
    "        Y = [p for (p, a) in zip(preds, actual) if not a]\n",
    "        return X, Y\n",
    "\n",
    "    def _compute_z_p(self):\n",
    "        X_A, Y_A = self._group_preds_by_label(self._preds1, self._label)\n",
    "        X_B, Y_B = self._group_preds_by_label(self._preds2, self._label)\n",
    "\n",
    "        V_A10, V_A01 = self._structural_components(X_A, Y_A)\n",
    "        V_B10, V_B01 = self._structural_components(X_B, Y_B)\n",
    "\n",
    "        auc_A = self._auc(X_A, Y_A)\n",
    "        auc_B = self._auc(X_B, Y_B)\n",
    "\n",
    "        # Compute entries of covariance matrix S (covar_AB = covar_BA)\n",
    "        var_A = (self._get_S_entry(V_A10, V_A10, auc_A, auc_A) * 1/len(V_A10)+ self._get_S_entry(V_A01, V_A01, auc_A, auc_A) * 1/len(V_A01))\n",
    "        var_B = (self._get_S_entry(V_B10, V_B10, auc_B, auc_B) * 1/len(V_B10)+ self._get_S_entry(V_B01, V_B01, auc_B, auc_B) * 1/len(V_B01))\n",
    "        covar_AB = (self._get_S_entry(V_A10, V_B10, auc_A, auc_B) * 1/len(V_A10)+ self._get_S_entry(V_A01, V_B01, auc_A, auc_B) * 1/len(V_A01))\n",
    "\n",
    "        # Two tailed test\n",
    "        z = self._z_score(var_A, var_B, covar_AB, auc_A, auc_B)\n",
    "        p = st.norm.sf(abs(z))*2\n",
    "\n",
    "        return z,p\n",
    "\n",
    "    def _show_result(self):\n",
    "        z,p=self._compute_z_p()\n",
    "        print(f\"z score = {z:.5f};\\np value = {p:.5f};\")\n",
    "        if p < self.threshold :print(\"There is a significant difference\")\n",
    "        else:        print(\"There is NO significant difference\")\n",
    "\n",
    "DelongTest(y_test_probs8[:,0],y_test_probs3[:,0],y_test1)\n",
    "DelongTest(y_test_probs8[:,0],y_test_probs4[:,0],y_test1)\n",
    "DelongTest(y_test_probs8[:,0],y_test_probs5[:,0],y_test1)\n",
    "DelongTest(y_test_probs8[:,1],y_test_probs3[:,1],y_test1)\n",
    "DelongTest(y_test_probs8[:,1],y_test_probs4[:,1],y_test1)\n",
    "DelongTest(y_test_probs8[:,1],y_test_probs5[:,1],y_test1)\n",
    "DelongTest(y_test_probs8[:,2],y_test_probs3[:,2],y_test1)\n",
    "DelongTest(y_test_probs8[:,2],y_test_probs4[:,2],y_test1)\n",
    "DelongTest(y_test_probs8[:,2],y_test_probs5[:,2],y_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train5 = label_binarize(y_train5, classes=[0, 1, 2])\n",
    "y_test5 = label_binarize(y_test5, classes=[0, 1, 2])\n",
    "auc5 = metrics.roc_auc_score(y_train5, y_train_probs5, average='macro', multi_class='ovr')\n",
    "auc_score5 = metrics.roc_auc_score(y_test5, y_test_probs5, average='macro', multi_class='ovr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=1.2)\n",
    "plt.style.use('seaborn-white')\n",
    "plt.figure(dpi=300)\n",
    "\n",
    "plt.plot(fprmacro_train8,tprmacro_train8,'#1f77b4', label='{}= {} ({}-{})'.format('Radiomic_macro_AUC', '%.3f' % auc8, '%.3f' % CI_macro_train8[0], '%.3f' % CI_macro_train8[1]))\n",
    "# plt.plot(fprmacro_train3,tprmacro_train3,'#ff7f0e', label='{}= {} ({}-{})'.format('Radiomic+miR371_macro_AUC', '%.3f' % auc3, '%.3f' % CI_macro_train3[0], '%.3f' % CI_macro_train3[1]))\n",
    "# plt.plot(fprmacro_train4,tprmacro_train4,'#2ca02c', label='{}= {} ({}-{})'.format('Radiomic+miR375_macro_AUC', '%.3f' % auc4, '%.3f' % CI_macro_train4[0], '%.3f' % CI_macro_train4[1]))\n",
    "plt.plot(fprmacro_train5,tprmacro_train5,'#d62728', label='{}= {} ({}-{})'.format('Radiomic+miR371+miR375_macro_AUC', '%.3f' % auc5, '%.3f' % CI_macro_train5[0], '%.3f' % CI_macro_train5[1]))\n",
    "\n",
    "plt.legend(loc='lower right',fontsize = 9)\n",
    "plt.plot([0,1],[0,1],'k--')\n",
    "plt.ylabel('Sensitivity',fontsize = 14)\n",
    "plt.xlabel('1 - Specificity',fontsize = 14)\n",
    "plt.savefig(\"C:/Users/Ding/Desktop/Figures and tables/三分类后融合1.svg\", dpi=300,format=\"svg\",bbox_inches = 'tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=1.2)\n",
    "plt.style.use('seaborn-white')\n",
    "plt.figure(dpi=300)\n",
    "\n",
    "plt.plot(fprmacro_test8,tprmacro_test8,'#1f77b4', label='{}= {} ({}-{})'.format('Radiomic_macro_AUC', '%.3f' % auc_score8, '%.3f' % CI_macro_test8[0], '%.3f' % CI_macro_test8[1]))\n",
    "# plt.plot(fprmacro_test3,tprmacro_test3,'#ff7f0e', label='{}= {} ({}-{})'.format('Radiomic+miR371_macro_AUC', '%.3f' % auc_score3, '%.3f' % CI_macro_test3[0], '%.3f' % CI_macro_test3[1]))\n",
    "# plt.plot(fprmacro_test4,tprmacro_test4,'#2ca02c', label='{}= {} ({}-{})'.format('Radiomic+miR375_macro_AUC', '%.3f' % auc_score4, '%.3f' % CI_macro_test4[0], '%.3f' % CI_macro_test4[1]))\n",
    "plt.plot(fprmacro_test5,tprmacro_test5,'#d62728', label='{}= {} ({}-{})'.format('Radiomic+miR371+miR375_macro_AUC', '%.3f' % auc_score5, '%.3f' % CI_macro_test5[0], '%.3f' % CI_macro_test5[1]))\n",
    "\n",
    "plt.legend(loc='lower right',fontsize = 9)\n",
    "plt.plot([0,1],[0,1],'k--')\n",
    "plt.ylabel('Sensitivity',fontsize = 14)\n",
    "plt.xlabel('1 - Specificity',fontsize = 14)\n",
    "plt.savefig(\"C:/Users/Ding/Desktop/Figures and tables/三分类后融合2.svg\", dpi=300,format=\"svg\",bbox_inches = 'tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "print(log_loss(y_train1,y_train_probs8))\n",
    "print(log_loss(y_train3,y_train_probs12))\n",
    "print(log_loss(y_train3,y_train_probs13))\n",
    "print(log_loss(y_train3,y_train_probs3))\n",
    "print(log_loss(y_train4,y_train_probs4))\n",
    "print(log_loss(y_train2,y_train_probs9))\n",
    "print(log_loss(y_train5,y_train_probs5))\n",
    "\n",
    "print(log_loss(y_test1,y_test_probs8))\n",
    "print(log_loss(y_test3,y_test_probs12))\n",
    "print(log_loss(y_test3,y_test_probs13))\n",
    "print(log_loss(y_test3,y_test_probs3))\n",
    "print(log_loss(y_test4,y_test_probs4))\n",
    "print(log_loss(y_test2,y_test_probs9))\n",
    "print(log_loss(y_test5,y_test_probs5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import brier_score_loss as BS\n",
    "y_train = label_binarize(y_train1, classes=[0, 1, 2])\n",
    "BS1_1=BS(y_train[:,0], y_train_probs8[:,0])\n",
    "BS1_2=BS(y_train[:,1], y_train_probs8[:,1])\n",
    "BS1_3=BS(y_train[:,2], y_train_probs8[:,2])\n",
    "\n",
    "BS2_1=BS(y_train[:,0], y_train_probs12[:,0])\n",
    "BS2_2=BS(y_train[:,1], y_train_probs12[:,1])\n",
    "BS2_3=BS(y_train[:,2], y_train_probs12[:,2])\n",
    "\n",
    "BS3_1=BS(y_train[:,0], y_train_probs13[:,0])\n",
    "BS3_2=BS(y_train[:,1], y_train_probs13[:,1])\n",
    "BS3_3=BS(y_train[:,2], y_train_probs13[:,2])\n",
    "\n",
    "BS4_1=BS(y_train[:,0], y_train_probs3[:,0])\n",
    "BS4_2=BS(y_train[:,1], y_train_probs3[:,1])\n",
    "BS4_3=BS(y_train[:,2], y_train_probs3[:,2])\n",
    "\n",
    "BS5_1=BS(y_train[:,0], y_train_probs4[:,0])\n",
    "BS5_2=BS(y_train[:,1], y_train_probs4[:,1])\n",
    "BS5_3=BS(y_train[:,2], y_train_probs4[:,2])\n",
    "\n",
    "BS6_1=BS(y_train[:,0], y_train_probs9[:,0])\n",
    "BS6_2=BS(y_train[:,1], y_train_probs9[:,1])\n",
    "BS6_3=BS(y_train[:,2], y_train_probs9[:,2])\n",
    "\n",
    "BS7_1=BS(y_train[:,0], y_train_probs5[:,0])\n",
    "BS7_2=BS(y_train[:,1], y_train_probs5[:,1])\n",
    "BS7_3=BS(y_train[:,2], y_train_probs5[:,2])\n",
    "print((BS1_1+BS1_2+BS1_3)/3)\n",
    "print((BS2_1+BS2_2+BS2_3)/3)\n",
    "print((BS3_1+BS3_2+BS3_3)/3)\n",
    "print((BS4_1+BS4_2+BS4_3)/3)\n",
    "print((BS5_1+BS5_2+BS5_3)/3)\n",
    "print((BS6_1+BS6_2+BS6_3)/3)\n",
    "print((BS7_1+BS7_2+BS7_3)/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = label_binarize(y_test1, classes=[0, 1, 2])\n",
    "BS1_1=BS(y_test[:,0], y_test_probs8[:,0])\n",
    "BS1_2=BS(y_test[:,1], y_test_probs8[:,1])\n",
    "BS1_3=BS(y_test[:,2], y_test_probs8[:,2])\n",
    "\n",
    "BS2_1=BS(y_test[:,0], y_test_probs12[:,0])\n",
    "BS2_2=BS(y_test[:,1], y_test_probs12[:,1])\n",
    "BS2_3=BS(y_test[:,2], y_test_probs12[:,2])\n",
    "\n",
    "BS3_1=BS(y_test[:,0], y_test_probs13[:,0])\n",
    "BS3_2=BS(y_test[:,1], y_test_probs13[:,1])\n",
    "BS3_3=BS(y_test[:,2], y_test_probs13[:,2])\n",
    "\n",
    "BS4_1=BS(y_test[:,0], y_test_probs3[:,0])\n",
    "BS4_2=BS(y_test[:,1], y_test_probs3[:,1])\n",
    "BS4_3=BS(y_test[:,2], y_test_probs3[:,2])\n",
    "\n",
    "BS5_1=BS(y_test[:,0], y_test_probs4[:,0])\n",
    "BS5_2=BS(y_test[:,1], y_test_probs4[:,1])\n",
    "BS5_3=BS(y_test[:,2], y_test_probs4[:,2])\n",
    "\n",
    "BS6_1=BS(y_test[:,0], y_test_probs9[:,0])\n",
    "BS6_2=BS(y_test[:,1], y_test_probs9[:,1])\n",
    "BS6_3=BS(y_test[:,2], y_test_probs9[:,2])\n",
    "\n",
    "BS7_1=BS(y_test[:,0], y_test_probs5[:,0])\n",
    "BS7_2=BS(y_test[:,1], y_test_probs5[:,1])\n",
    "BS7_3=BS(y_test[:,2], y_test_probs5[:,2])\n",
    "print((BS1_1+BS1_2+BS1_3)/3)\n",
    "print((BS2_1+BS2_2+BS2_3)/3)\n",
    "print((BS3_1+BS3_2+BS3_3)/3)\n",
    "print((BS4_1+BS4_2+BS4_3)/3)\n",
    "print((BS5_1+BS5_2+BS5_3)/3)\n",
    "print((BS6_1+BS6_2+BS6_3)/3)\n",
    "print((BS7_1+BS7_2+BS7_3)/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "# calibrated_clf = CalibratedClassifierCV(model_XGB, cv=\"prefit\", method=\"sigmoid\")\n",
    "# calibrated_clf.fit(X_train, y_train)\n",
    "# y_test_probs = calibrated_clf.predict_proba(X_test)\n",
    "\n",
    "plt.figure(figsize=(7, 7))\n",
    "dpi=300\n",
    "ax1 = plt.subplot2grid((3, 1), (0, 0), rowspan=2)\n",
    "ax1.plot([0, 1], [0, 1], \"k:\", label=\"Perfectly calibrated\")\n",
    "y_test1 = label_binarize(y_test1, classes=[0, 1, 2])\n",
    "y_test3 = label_binarize(y_test3, classes=[0, 1, 2])\n",
    "# y_test4 = label_binarize(y_test4, classes=[0, 1, 2])\n",
    "y_test5 = label_binarize(y_test5, classes=[0, 1, 2])\n",
    "\n",
    "y_test_probs8 = (y_test_probs8 - y_test_probs8.min()) / (y_test_probs8.max() - y_test_probs8.min())\n",
    "fraction_of_positives, mean_predicted_value = calibration_curve(y_test1[:,1], y_test_probs8[:,1], n_bins=4,strategy='quantile')\n",
    "ax1.plot(mean_predicted_value, fraction_of_positives, \"o-\",label=\"%s\" % (\"Radiomic\", ))\n",
    "\n",
    "# y_test_probs3 = (y_test_probs3 - y_test_probs3.min()) / (y_test_probs3.max() - y_test_probs3.min())\n",
    "# fraction_of_positives, mean_predicted_value = calibration_curve(y_test3[:,1], y_test_probs3[:,1], n_bins=3,strategy='quantile')\n",
    "# ax1.plot(mean_predicted_value, fraction_of_positives, \"s-\",label=\"%s\" % (\"Radiomic+miR371\", ))\n",
    "\n",
    "# y_test_probs4 = (y_test_probs4 - y_test_probs4.min()) / (y_test_probs4.max() - y_test_probs4.min())\n",
    "# fraction_of_positives, mean_predicted_value = calibration_curve(y_test3[:,1], y_test_probs4[:,1], n_bins=3,strategy='quantile')\n",
    "# ax1.plot(mean_predicted_value, fraction_of_positives, \"^-\",label=\"%s\" % (\"Radiomic+miR375\", ))\n",
    "\n",
    "y_test_probs5 = (y_test_probs5 - y_test_probs5.min()) / (y_test_probs5.max() - y_test_probs5.min())\n",
    "fraction_of_positives, mean_predicted_value = calibration_curve(y_test3[:,1], y_test_probs5[:,1], n_bins=4,strategy='quantile')\n",
    "ax1.plot(mean_predicted_value, fraction_of_positives, \"D-\",label=\"%s\" % (\"Radiomic+miR371+miR375\", ))\n",
    "ax1.set_ylabel(\"Fraction of positives\",size='18')\n",
    "ax1.set_ylim([-0.05, 1.05])\n",
    "plt.xticks(size='18')\n",
    "plt.yticks(size='18')\n",
    "ax1.set_xlabel(\"Predicted probability\",size='18')\n",
    "ax1.legend(loc=\"lower right\",fontsize = 14)\n",
    "ax1.set_title('Calibration plots  (reliability curve)',size='18')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"C:/Users/Ding/Desktop/Figures and tables/三分类校准曲线.svg\", dpi=300,format=\"svg\",bbox_inches = 'tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "# calibrated_clf = CalibratedClassifierCV(model_XGB, cv=\"prefit\", method=\"sigmoid\")\n",
    "# calibrated_clf.fit(X_train, y_train)\n",
    "# y_test_probs = calibrated_clf.predict_proba(X_test)\n",
    "\n",
    "plt.figure(figsize=(7, 7))\n",
    "dpi=300\n",
    "ax1 = plt.subplot2grid((3, 1), (0, 0), rowspan=2)\n",
    "ax1.plot([0, 1], [0, 1], \"k:\", label=\"Perfectly calibrated\")\n",
    "y_test1 = label_binarize(y_test1, classes=[0, 1, 2])\n",
    "# y_test3 = label_binarize(y_test3, classes=[0, 1, 2])\n",
    "# y_test4 = label_binarize(y_test4, classes=[0, 1, 2])\n",
    "y_test5 = label_binarize(y_test5, classes=[0, 1, 2])\n",
    "\n",
    "y_test_probs8 = (y_test_probs8 - y_test_probs8.min()) / (y_test_probs8.max() - y_test_probs8.min())\n",
    "fraction_of_positives, mean_predicted_value = calibration_curve(y_test1[:,2], y_test_probs8[:,2], n_bins=4,strategy='quantile')\n",
    "ax1.plot(mean_predicted_value, fraction_of_positives, \"o-\",label=\"%s\" % (\"Radiomic\", ))\n",
    "\n",
    "# y_test_probs3 = (y_test_probs3 - y_test_probs3.min()) / (y_test_probs3.max() - y_test_probs3.min())\n",
    "# fraction_of_positives, mean_predicted_value = calibration_curve(y_test3[:,2], y_test_probs3[:,2], n_bins=3,strategy='quantile')\n",
    "# ax1.plot(mean_predicted_value, fraction_of_positives, \"s-\",label=\"%s\" % (\"Radiomic+miR371\", ))\n",
    "\n",
    "# y_test_probs4 = (y_test_probs4 - y_test_probs4.min()) / (y_test_probs4.max() - y_test_probs4.min())\n",
    "# fraction_of_positives, mean_predicted_value = calibration_curve(y_test3[:,2], y_test_probs4[:,2], n_bins=3,strategy='quantile')\n",
    "# ax1.plot(mean_predicted_value, fraction_of_positives, \"^-\",label=\"%s\" % (\"Radiomic+miR375\", ))\n",
    "\n",
    "y_test_probs5 = (y_test_probs5 - y_test_probs5.min()) / (y_test_probs5.max() - y_test_probs5.min())\n",
    "fraction_of_positives, mean_predicted_value = calibration_curve(y_test3[:,2], y_test_probs5[:,2], n_bins=4,strategy='quantile')\n",
    "ax1.plot(mean_predicted_value, fraction_of_positives, \"D-\",label=\"%s\" % (\"Radiomic+miR371+miR375\", ))\n",
    "ax1.set_ylabel(\"Fraction of positives\",size='18')\n",
    "ax1.set_ylim([-0.05, 1.05])\n",
    "plt.xticks(size='18')\n",
    "plt.yticks(size='18')\n",
    "ax1.set_xlabel(\"Predicted probability\",size='18')\n",
    "ax1.legend(loc=\"lower right\",fontsize = 14)\n",
    "ax1.set_title('Calibration plots  (reliability curve)',size='18')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"C:/Users/Ding/Desktop/Figures and tables/三分类校准曲线2.svg\", dpi=300,format=\"svg\",bbox_inches = 'tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import chi2\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# 对预测概率分组\n",
    "def Hosmer_Lemeshow_test(y, y_pred, num_groups):\n",
    "    grouped_observed = np.array_split(y, num_groups)  \n",
    "    grouped_predicted = np.array_split(y_pred, num_groups) \n",
    "\n",
    "    # 计算每组的观察值和预测值比例\n",
    "    observed_ratio = [np.mean(g) for g in grouped_observed]\n",
    "    predicted_ratio = [np.mean(g) for g in grouped_predicted]\n",
    "\n",
    "    # 执行Hosmer-Lemeshow检验\n",
    "    hl_statistic = np.sum(((o - p) ** 2) / (p * (1 - p)) for o, p in zip(observed_ratio, predicted_ratio))\n",
    "    hl_p_value = chi2.sf(hl_statistic, num_groups - 2)\n",
    "    print(\"Hosmer-Lemeshow statistic:\", hl_statistic)\n",
    "    print(\"p-value:\", hl_p_value)\n",
    "Hosmer_Lemeshow_test(y_train1,y_train_probs8,4)\n",
    "Hosmer_Lemeshow_test(y_train1,y_train_probs12,4)\n",
    "Hosmer_Lemeshow_test(y_train1,y_train_probs13,4)\n",
    "Hosmer_Lemeshow_test(y_train3,y_train_probs3,4)\n",
    "Hosmer_Lemeshow_test(y_train4,y_train_probs4,4)\n",
    "Hosmer_Lemeshow_test(y_train1,y_train_probs9,4)\n",
    "Hosmer_Lemeshow_test(y_train5,y_train_probs5,4)\n",
    "\n",
    "Hosmer_Lemeshow_test(y_test1,y_test_probs8,4)\n",
    "Hosmer_Lemeshow_test(y_test1,y_test_probs12,4)\n",
    "Hosmer_Lemeshow_test(y_test1,y_test_probs13,4)\n",
    "Hosmer_Lemeshow_test(y_test3,y_test_probs3,4)\n",
    "Hosmer_Lemeshow_test(y_test4,y_test_probs4,4)\n",
    "Hosmer_Lemeshow_test(y_test1,y_test_probs9,4)\n",
    "Hosmer_Lemeshow_test(y_test5,y_test_probs5,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "def calculate_net_benefit_model(thresh_group, y_pred_score, y_label):\n",
    "    net_benefit_model = np.array([])\n",
    "    for thresh in thresh_group:\n",
    "        y_pred_label = y_pred_score > thresh\n",
    "        tn, fp, fn, tp = confusion_matrix(y_label, y_pred_label).ravel()\n",
    "        n = len(y_label)\n",
    "        net_benefit = (tp / n) - (fp / n) * (thresh / (1 - thresh))\n",
    "        net_benefit_model = np.append(net_benefit_model, net_benefit)\n",
    "    return net_benefit_model\n",
    "\n",
    "\n",
    "def calculate_net_benefit_all(thresh_group, y_label):\n",
    "    net_benefit_all = np.array([])\n",
    "    tn, fp, fn, tp = confusion_matrix(y_label, y_label).ravel()\n",
    "    total = tp + tn\n",
    "    for thresh in thresh_group:\n",
    "        net_benefit = (tp / total) - (tn / total) * (thresh / (1 - thresh))\n",
    "        net_benefit_all = np.append(net_benefit_all, net_benefit)\n",
    "    return net_benefit_all\n",
    "\n",
    "\n",
    "def plot_DCA(ax, thresh_group, net_benefit_model1, net_benefit_model4, net_benefit_all):\n",
    "    #Plot\n",
    "    ax.plot(thresh_group, net_benefit_model1, color = '#1f77b4', label = 'Radiomic')\n",
    "#     ax.plot(thresh_group, net_benefit_model2, color = '#ff7f0e', label = 'Radiomic+miR371')\n",
    "#     ax.plot(thresh_group, net_benefit_model3, color = '#2ca02c', label = 'Radiomic+miR375')\n",
    "    ax.plot(thresh_group, net_benefit_model4, color = '#d62728', label = 'Radiomic+miR371+miR375')\n",
    "    ax.plot(thresh_group, net_benefit_all, color = 'black',label = 'Treat all')\n",
    "    ax.plot((0, 1), (0, 0), color = 'black', linestyle = ':', label = 'Treat none')\n",
    "\n",
    "    #Fill，显示出模型较于treat all和treat none好的部分\n",
    "    y2 = np.maximum(net_benefit_all, 0)\n",
    "#     y3 = np.maximum(net_benefit_model2, y2)\n",
    "    y1 = np.maximum(net_benefit_model1, y2)\n",
    "#     y4 = np.maximum(net_benefit_model3, y2)\n",
    "    y5 = np.maximum(net_benefit_model4, y2)\n",
    "    ax.fill_between(thresh_group, y1, y2, color = '#1f77b4', alpha = 0.3)\n",
    "#     ax.fill_between(thresh_group, y3, y1, color = '#ff7f0e', alpha = 0.3)\n",
    "#     ax.fill_between(thresh_group, y4, y3, color = '#2ca02c', alpha = 0.3)\n",
    "    ax.fill_between(thresh_group, y5, y1, color = '#d62728', alpha = 0.3)\n",
    "\n",
    "    #Figure Configuration， 美化一下细节\n",
    "    ax.set_xlim(0,0.91)\n",
    "    ax.set_ylim(net_benefit_model1.min() - 0.1 , net_benefit_model1.max() + 0.1)#adjustify the y axis limitation\n",
    "    ax.set_xlabel(\n",
    "        xlabel = 'Threshold Probability', \n",
    "        fontdict= {'family': 'Arial', 'fontsize': 18}\n",
    "        )\n",
    "    ax.set_ylabel(\n",
    "        ylabel = 'Net Benefit', \n",
    "        fontdict= {'family': 'Arial', 'fontsize': 18}\n",
    "        )\n",
    "    plt.xticks(size='18')\n",
    "    plt.yticks(size='18')\n",
    "    dpi=300\n",
    "    ax.grid('major')\n",
    "    ax.spines['right'].set_color((0.8, 0.8, 0.8))\n",
    "    ax.spines['top'].set_color((0.8, 0.8, 0.8)) \n",
    "    ax.legend(loc = 'upper right',fontsize = 12 )\n",
    "\n",
    "    return ax\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    y_pred_score8 = y_test_probs8[:,1]\n",
    "    y_label1 = y_test1[:,1]\n",
    "#     y_pred_score3 = y_test_probs3[:,1]\n",
    "#     y_label2 = y_test3[:,1]\n",
    "#     y_pred_score4 = y_test_probs4[:,1]\n",
    "#     y_label3 = y_test3[:,1]\n",
    "    y_pred_score5 = y_test_probs5[:,1]\n",
    "    y_label4 = y_test3[:,1]\n",
    "    thresh_group = np.arange(0,1,0.05)\n",
    "    net_benefit_model1 = calculate_net_benefit_model(thresh_group, y_pred_score8, y_label1)\n",
    "#     net_benefit_model2 = calculate_net_benefit_model(thresh_group, y_pred_score3, y_label2)\n",
    "#     net_benefit_model3 = calculate_net_benefit_model(thresh_group, y_pred_score4, y_label3)\n",
    "    net_benefit_model4 = calculate_net_benefit_model(thresh_group, y_pred_score5, y_label4)\n",
    "    net_benefit_all = calculate_net_benefit_all(thresh_group, y_label1)\n",
    "    fig, ax = plt.subplots()\n",
    "    ax = plot_DCA(ax, thresh_group, net_benefit_model1, net_benefit_model4, net_benefit_all)\n",
    "    ax.set_title('Decision curve',size='18')\n",
    "    plt.savefig(\"C:/Users/Ding/Desktop/Figures and tables/三分类决策曲线.svg\", dpi=300,format=\"svg\",bbox_inches = 'tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_DCA(ax, thresh_group, net_benefit_model1, net_benefit_model4, net_benefit_all):\n",
    "    #Plot\n",
    "    ax.plot(thresh_group, net_benefit_model1, color = '#1f77b4', label = 'Radiomic')\n",
    "#     ax.plot(thresh_group, net_benefit_model2, color = '#ff7f0e', label = 'Radiomic+miR371')\n",
    "#     ax.plot(thresh_group, net_benefit_model3, color = '#2ca02c', label = 'Radiomic+miR375')\n",
    "    ax.plot(thresh_group, net_benefit_model4, color = '#d62728', label = 'Radiomic+miR371+miR375')\n",
    "    ax.plot(thresh_group, net_benefit_all, color = 'black',label = 'Treat all')\n",
    "    ax.plot((0, 1), (0, 0), color = 'black', linestyle = ':', label = 'Treat none')\n",
    "\n",
    "    #Fill，显示出模型较于treat all和treat none好的部分\n",
    "    y2 = np.maximum(net_benefit_all, 0)\n",
    "#     y3 = np.maximum(net_benefit_model2, y2)\n",
    "    y1 = np.maximum(net_benefit_model1, y2)\n",
    "#     y4 = np.maximum(net_benefit_model3, y2)\n",
    "    y5 = np.maximum(net_benefit_model4, y2)\n",
    "    ax.fill_between(thresh_group, y1, y2, color = '#1f77b4', alpha = 0.3)\n",
    "#     ax.fill_between(thresh_group, y4, y1, color = '#ff7f0e', alpha = 0.3)\n",
    "#     ax.fill_between(thresh_group, y3, y4, color = '#2ca02c', alpha = 0.3)\n",
    "    ax.fill_between(thresh_group, y5, y1, color = '#d62728', alpha = 0.3)\n",
    "\n",
    "    #Figure Configuration， 美化一下细节\n",
    "    ax.set_xlim(0,0.79)\n",
    "    ax.set_ylim(net_benefit_model1.min() + 0 , net_benefit_model1.max() + 0.1)#adjustify the y axis limitation\n",
    "    ax.set_xlabel(\n",
    "        xlabel = 'Threshold Probability', \n",
    "        fontdict= {'family': 'Arial', 'fontsize': 18}\n",
    "        )\n",
    "    ax.set_ylabel(\n",
    "        ylabel = 'Net Benefit', \n",
    "        fontdict= {'family': 'Arial', 'fontsize': 18}\n",
    "        )\n",
    "    plt.xticks(size='18')\n",
    "    plt.yticks(size='18')\n",
    "    dpi=300\n",
    "    ax.grid('major')\n",
    "    ax.spines['right'].set_color((0.8, 0.8, 0.8))\n",
    "    ax.spines['top'].set_color((0.8, 0.8, 0.8)) \n",
    "    ax.legend(loc = 'upper right',fontsize = 12 )\n",
    "\n",
    "    return ax\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    y_pred_score8 = y_test_probs8[:,2]\n",
    "    y_label1 = y_test1[:,2]\n",
    "#     y_pred_score3 = y_test_probs3[:,2]\n",
    "#     y_label2 = y_test3[:,2]\n",
    "#     y_pred_score4 = y_test_probs4[:,2]\n",
    "#     y_label3 = y_test3[:,2]\n",
    "    y_pred_score5 = y_test_probs5[:,2]\n",
    "    y_label4 = y_test3[:,2]\n",
    "    thresh_group = np.arange(0,1,0.05)\n",
    "    net_benefit_model1 = calculate_net_benefit_model(thresh_group, y_pred_score8, y_label1)\n",
    "#     net_benefit_model2 = calculate_net_benefit_model(thresh_group, y_pred_score3, y_label2)\n",
    "#     net_benefit_model3 = calculate_net_benefit_model(thresh_group, y_pred_score4, y_label3)\n",
    "    net_benefit_model4 = calculate_net_benefit_model(thresh_group, y_pred_score5, y_label4)\n",
    "    net_benefit_all = calculate_net_benefit_all(thresh_group, y_label1)\n",
    "    fig, ax = plt.subplots()\n",
    "    ax = plot_DCA(ax, thresh_group, net_benefit_model1, net_benefit_model4, net_benefit_all)\n",
    "    ax.set_title('Decision curve',size='18')\n",
    "    plt.savefig(\"C:/Users/Ding/Desktop/Figures and tables/三分类决策曲线2.svg\", dpi=300,format=\"svg\",bbox_inches = 'tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import norm\n",
    "import numpy as np\n",
    "from plotnine import *\n",
    "\"\"\"因变量y必须是int：0或1\"\"\"\n",
    "def logistic_regression(y,x,df):\n",
    "    model = smf.glm(formula = f'{y} ~ {x}',\n",
    "                    data = df,\n",
    "                    family=sm.families.Binomial()).fit()  \n",
    "    print(model.summary(), end='\\n\\n\\n')\n",
    "    stat = pd.DataFrame({'p': model.pvalues, # series：每个β的p值\n",
    "                         # 'beta': model.params, # series：β\n",
    "                         # 'beta_lower_ci': model.conf_int().iloc[:, 0], #df：β的95%CI\n",
    "                         # 'beta_upper_ci': model.conf_int().iloc[:, 1], #df：β的95%CI                        \n",
    "                         'OR': np.exp(model.params), # series：OR\n",
    "                         'OR_lower_ci': np.exp(model.params - norm.ppf(0.975)*model.bse),\n",
    "                         'OR_upper_ci': np.exp(model.params + norm.ppf(0.975)*model.bse)}) \n",
    "    stat['sig'] = stat.apply(lambda x : \"*\" if x['p']<0.05 else \"no_sig\",axis=1)\n",
    "    stat= stat.sort_values('OR', ascending=True)                            \n",
    "    print(stat)\n",
    "    \"\"\"绘制森林图：OR\"\"\"\n",
    "    forest_df = stat.drop(\"Intercept\")\\\n",
    "                    .reset_index()\\\n",
    "                    .rename(columns={'index': 'independent_var'})\\\n",
    "                    .sort_values('OR', ascending=False)\n",
    "    print(ggplot(forest_df , aes(y='independent_var', x='OR')) + # y轴（自变量x）为连续变量或二分类变量\n",
    "          geom_point(aes(color='sig'),size=2) +          \n",
    "          geom_errorbarh(aes(xmin='OR_lower_ci', \n",
    "                             xmax='OR_upper_ci',\n",
    "                             color ='sig'), height=0.1) +\n",
    "          scale_color_manual(values = [\"red\",\"black\"]) +\n",
    "          scale_y_discrete(limits= forest_df[\"independent_var\"]) +\n",
    "          guides(color=guide_legend(reverse=True))+\n",
    "          labs(title='logistic Regression', x='OR', y='variable')+\n",
    "          geom_vline(xintercept=1, linetype='dashed', color='black')+\n",
    "          theme_minimal()+\n",
    "          theme(plot_title=element_text(hjust=0.5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('C:/Users/Ding/Desktop/融合/三分类融合/trainCT+371+375.csv')\n",
    "logistic_regression(y=\"N\",x=\"Radscore\",df=df)\n",
    "logistic_regression(y=\"T\",x=\"Radscore\",df=df)\n",
    "logistic_regression(y=\"V\",x=\"Radscore\",df=df)\n",
    "logistic_regression(y=\"N\",x=\"miR371+miR375\",df=df)\n",
    "logistic_regression(y=\"T\",x=\"miR371+miR375\",df=df)\n",
    "logistic_regression(y=\"V\",x=\"miR371+miR375\",df=df)\n",
    "logistic_regression(y=\"N\",x=\"Radscore+miR371\",df=df)\n",
    "logistic_regression(y=\"T\",x=\"Radscore+miR371\",df=df)\n",
    "logistic_regression(y=\"V\",x=\"Radscore+miR371\",df=df)\n",
    "logistic_regression(y=\"N\",x=\"Radscore+miR375\",df=df)\n",
    "logistic_regression(y=\"T\",x=\"Radscore+miR375\",df=df)\n",
    "logistic_regression(y=\"V\",x=\"Radscore+miR375\",df=df)\n",
    "logistic_regression(y=\"N\",x=\"Radscore+miR371+miR375\",df=df)\n",
    "logistic_regression(y=\"T\",x=\"Radscore+miR371+miR375\",df=df)\n",
    "logistic_regression(y=\"V\",x=\"Radscore+miR371+miR375\",df=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#坐标轴负号的处理\n",
    "plt.rcParams['axes.unicode_minus']=False\n",
    "# 读取数据\n",
    "tips = pd.read_csv('C:/Users/Ding/Desktop/融合/三分类融合/train标准化1.csv')\n",
    "# 绘制分组小提琴图\n",
    "sns.boxplot(x = \"group1\", # 指定x轴的数据\n",
    "               y = \"Texture feature parameter\", # 指定y轴的数据\n",
    "               hue = \"group2\", # 指定分组变量\n",
    "               data = tips, # 指定绘图的数据集\n",
    "               order = ['original_shape_Flatness','original_ngtdm_Contrast','wavelet-LHL_glcm_Imc2','wavelet-LHL_gldm_DependenceVariance',\n",
    "         'wavelet-LLL_gldm_DependenceEntropy','log-sigma-3-mm-3D_glszm_SmallAreaEmphasis'], # 指定x轴刻度标签的顺序\n",
    "               palette = 'Set2', # 指定不同性别对应的颜色（因为hue参数为设置为性别变量）\n",
    "               saturation=1, #饱和度\n",
    "               whis=1,\n",
    "               #flierprops = {'marker': 'D'},  # 异常值形状 'markerfacecolor': 'red',  # 形状填充色\n",
    "               fliersize=4, \n",
    "               whiskerprops={'linestyle':'-', 'color':'black'},  # 设置上下须属性\n",
    "               showmeans=True,  # 箱图显示均值，\n",
    "               meanprops={'marker': 's','markerfacecolor':'w', 'markeredgecolor':'w','fillstyle':'full','markersize': 5}  # 设置均值属性\n",
    "              )\n",
    "# 设置图例\n",
    "plt.legend(loc = 'upper right', ncol = 1)\n",
    "plt.xticks(rotation=45, ha = 'right',va = 'top')\n",
    "plt.xlabel('')\n",
    "# sns.swarmplot(x='group1', y='rad',data=tips,color='w', alpha=0.8, size=3)\n",
    "# 显示图形\n",
    "plt.savefig(\"C:/Users/Ding/Desktop/Figures and tables/三分类特征对比1.svg\", dpi=300,format=\"svg\",bbox_inches = 'tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#坐标轴负号的处理\n",
    "plt.rcParams['axes.unicode_minus']=False\n",
    "# 读取数据\n",
    "tips = pd.read_csv('C:/Users/Ding/Desktop/融合/三分类融合/test标准化1.csv')\n",
    "# 绘制分组小提琴图\n",
    "sns.boxplot(x = \"group1\", # 指定x轴的数据\n",
    "               y = \"Texture feature parameter\", # 指定y轴的数据\n",
    "               hue = \"group2\", # 指定分组变量\n",
    "               data = tips, # 指定绘图的数据集\n",
    "               order = ['original_shape_Flatness','original_ngtdm_Contrast','wavelet-LHL_glcm_Imc2','wavelet-LHL_gldm_DependenceVariance',\n",
    "         'wavelet-LLL_gldm_DependenceEntropy','log-sigma-3-mm-3D_glszm_SmallAreaEmphasis'], # 指定x轴刻度标签的顺序\n",
    "               palette = 'Set2', # 指定不同性别对应的颜色（因为hue参数为设置为性别变量）\n",
    "               saturation=1, #饱和度\n",
    "               whis=1,\n",
    "               #flierprops = {'marker': 'D'},  # 异常值形状 'markerfacecolor': 'red',  # 形状填充色\n",
    "               fliersize=4, \n",
    "               whiskerprops={'linestyle':'-', 'color':'black'},  # 设置上下须属性\n",
    "               showmeans=True,  # 箱图显示均值，\n",
    "               meanprops={'marker': 's','markerfacecolor':'w', 'markeredgecolor':'w','fillstyle':'full','markersize': 5}  # 设置均值属性\n",
    "              )\n",
    "# 设置图例\n",
    "plt.legend(loc = 'upper right', ncol = 1)\n",
    "plt.xticks(rotation=45, ha = 'right',va = 'top')\n",
    "plt.xlabel('')\n",
    "# sns.swarmplot(x='group1', y='rad',data=tips,color='w', alpha=0.8, size=3)\n",
    "# 显示图形\n",
    "plt.savefig(\"C:/Users/Ding/Desktop/Figures and tables/三分类特征对比2.svg\", dpi=300,format=\"svg\",bbox_inches = 'tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
